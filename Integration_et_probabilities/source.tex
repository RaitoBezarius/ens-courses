% !TeX spellcheck = fr-FR


\documentclass[10pt,a4paper,notitlepage ]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{tcolorbox}
\usepackage{stmaryrd}
\usepackage[left=3.00cm, right=3.00cm, top=1.50cm, bottom=1.50cm]{geometry}


\title{Intégration et probabilités}
\date{}

\newcommand{\N}{\mathbb N}
\newcommand{\R}{\mathbb R}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Z}{\mathbb Z}
\newcommand{\T}{\mathcal T}
\newcommand{\A}{\mathcal A}
\newcommand{\B}{\mathcal B}
\newcommand{\eps}{\varepsilon}
\newcommand{\comp}[1]{#1^\complement}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\indep}{\perp \!\!\! \perp}
\renewcommand{\Im}{\mathrm{Im }}

\newenvironment{definition}{
	
	\textbf{Définition : }
}
{}

\newcounter{th}
\newenvironment{theorem}[1][]{
\refstepcounter{th}
\begin{tcolorbox}
	\textbf{Théorème \theth \ #1}
	
	
}{\end{tcolorbox}}

\newenvironment{propriete}[1][]{
	\begin{tcolorbox}
		\textbf{Propriété #1 : }
}
{\end{tcolorbox}}

\newenvironment{demo}[1][]{

	\textbf{Démonstration #1 :}
}{\begin{flushright}
	$\square$
\end{flushright}
}

\newenvironment{exo}{
	
	\textbf{Exercice :} }{}

\newenvironment{exemple}{
	
	\textbf{Exemple :} }{}

\newenvironment{corollaire}{
	\begin{tcolorbox}
		\textbf{Corollaire : }
	}
	{\end{tcolorbox}}

\newenvironment{lemme}[1][]{
	\begin{tcolorbox}
		\textbf{Lemme #1 : }
	}
	{\end{tcolorbox}}

\newenvironment{rem}{
	
	\textbf{Remarque :}}{}

\begin{document}
	\maketitle
	\part*{Introduction}
	Références : 
	\begin{itemize}
		\item \textsc{Billingsley}, \textit{Probabiliy and measure}
		\item \textsc{Kolmogorov \& Fomin}, tome 2 
	\end{itemize}
	Motivations :
	\begin{itemize}
		\item Définir la longueur d'une partie de $\mathbb R$
		\item Définir l'aire d'une partie de $\mathbb R ^2$
		\item  Définir $\int f \mathrm dx$ pour $f : \mathbb R ^d \rightarrow \mathbb R$
		\item Définir, préciser la notion mathématique décrivant une suite infinie de jets de dés
	\end{itemize}
Par exemple :
\begin{itemize}
	\item Si $f : \mathbb R \rightarrow \mathbb R$, on peut définir $\int f$ comme l'aire algébrique définie par le graphe de $f$. Ainsi, définir une aire permet de définir une intégrale
	\item De même, $\lambda(A) = \mathds 1_A$ avec $\mathds 1_A(x)=1$ ssi $x \in A$. Donc définir une intégrale revient à définir une mesure.
	\item Tirer un nombre au hasard dans $[\,0,1]\,$, cela revient à tirer au hasard la suite de ses décimales au D10, car on mesure une partie de $\{ 0, 1, \dots 9 \}^\mathbb N$
 \end{itemize}

On se demande alors comment définir la surface d'une partie du plan.

Méthode 1 : à la Riemann. On approxime avec un quadrillage. On compte le nombre de carrés qui intersectent l'ensemble considéré, puis on conclut en passant à la limite quand le côté du quadrillage tend vers $0$.

Méthode 2 : on pose $\lambda(A) := \underset{(R_i)}{\inf} \sum_{i=1}^{\infty}  \lambda(R_i)$ où $R_i$ est une suite de rectangles recouvrant $A$.

À noter : les deux méthodes ont des cas pathologiques différents.

\part*{Ensembles dénombrables}

\begin{definition}
	Un ensemble est dénombrable ssi il est en bijection avec $\mathbb N$
\end{definition}
\begin{propriete}
	Toute partie d'un ensemble dénombrable est au plus dénombrable
\end{propriete}
\begin{demo}
	On pose $x : \mathbb N \rightarrow X, Y \subset X$. Si $Y$ n'est pas fini :
	\begin{align*}
	&i_1 = \min \{i\in\mathbb N, x_i \in Y \} \\
	&\dots  \\
 	&i_n = \min \{ i\in \mathbb N, x_i\in Y \setminus\{x_1, \dots,x_{n-1}\}\}
 	\end{align*} 
	Ainsi, $k \mapsto x_{n_k}$ est une bijection de $\mathbb N$ vers $Y$.
\end{demo}

\begin{propriete}
	L'image d'une suite est au plus dénombrable.
\end{propriete}
\begin{demo}
	On note $x:\mathbb N \rightarrow X$ une suite.
On crée de manière analogue une sous-suite injective de $x$ de même image que $x$ (sauf si $f(x(\mathbb N))$ est fini).
\end{demo}
\begin{propriete}
	$\mathbb N \times \mathbb N$ est dénombrable.
\end{propriete}
\begin{demo}
	$(n_1,n_2) \mapsto 2^{n_1} (2n_2+1) -1$ est une bijection $\mathbb N^2 \rightarrow \mathbb N$.
\end{demo}

\begin{propriete}
	Une réunion au plus dénombrables d'ensembles au plus dénombrable est au plus dénombrable.
\end{propriete}

\begin{demo}
	On traite le cas "union dénombrable d'ensembles dénombrables".
	
	Soit $A_i$ des parties dénombrables d'un ensemble $X$.
	Pour tout $i$, il existe $b_i : \mathbb N \rightarrow A_i$ bijection. (nb : ceci requiert en fait l'axiome du choix dénombrable)
	Alors$ \begin{aligned}
		(i,j) &\mapsto b_i(j) \\
		\mathbb N ^2 & \rightarrow \underset{i}{\bigcup} A_i
	\end{aligned}$ est surjective.

Donc $\underset{i}{\bigcup}A_i$ est au plus dénombrable.

Or $\underset{i}{\bigcup}A_i \supset A_i$.

Donc $\underset{i}{\bigcup} A_i$ est dénombrable.
\end{demo}

\begin{propriete}
	Si $X$ est dénombrable, $\mathcal P(X)$ ne l'est pas.
	
	Plus généralement, quel que soit $X$, $X$ et $\mathcal P(X)$ ne sont jamais en bijection (théorème de Cantor).
\end{propriete}

\begin{demo}
	Supposons qu'il existe $x: \begin{aligned} X & \rightarrow \mathcal P(X) \\
		x & \mapsto A_x \end{aligned}$ une bijection.
	
	Considérons $B := \{x, x\notin A_x\}$. Comme $x$ est une bijection, il existe $y \in X$ tel que $B = A_y$.
	
	Question : a-t-on $y\in B$. On arrive à un paradoxe type Russel.
\end{demo}

\begin{exo}
\begin{itemize}	
	\item $\{0,1\}^\mathbb N$ est non dénombrable.
	
	\item $\mathbb R$ est non dénombrable.
	
\end{itemize}
\end{exo}

\part*{$\limsup$ et $\liminf$}

\begin{definition}
	
Soit $(x_n)_{n\in\mathbb N} \in \mathbb R ^\mathbb N$ (plus généralement $\in\bar{\mathbb R}^\mathbb N$). Alors $s_n := \underset{k\geq n}{\sup} x_k$.

$s_n$ est décroissante (donc a une limite dans $\bar{\mathbb R})$.

Alors $\lim s_n =: \limsup x_n = \inf s_n$.

De même pour $\liminf x_n$. 
\end{definition}
\begin{propriete}
	$\lim x_n$ existe ssi $\liminf x_n = \limsup x_n$. Dans ce cas, $\lim x_n = \limsup x_n = \liminf x_n$.
\end{propriete}

\begin{demo}
	$\Leftarrow$ : $i_n \leq x_n \leq s_n$. On conclut par théorème d'encadrement.
	
	$\Rightarrow$ : Si $x_n \rightarrow l$ alors : $\forall \varepsilon > 0, \exists N \in \mathbb N, \forall n \geq N, l-\varepsilon \leq i_n \leq l \leq s_n \leq l+\varepsilon$.
	Donc $s_n \rightarrow l$ et $i_n \rightarrow l$.
\end{demo}

\begin{propriete}
	Si $y_n$ est une sous-suite de $x_n$, alors $\liminf x_n \le \liminf y_n \le \limsup y_n \le \limsup x_n$
\end{propriete}

Ainsi, si $l$ est valeur d'adhérence de $x_n$, alors $\liminf x_n \le l \le \limsup x_n$.
\begin{propriete}
	$\limsup x_n = -\liminf (-x_n)$
\end{propriete}

\begin{propriete}
	Il existe une sous-suite de $x_n$ qui converge vers $\limsup x_n$. Idem pour $\liminf x_n$.
\end{propriete}
\begin{demo}
	On choisit $k_n \ge n$ tel que $s_n - \frac{1}{n} \le x_{k_n} \le s_n$. $n \mapsto x_{k_n}$ converge vers $\limsup x_n$.
\end{demo}

\part*{Familles sommables}

On pose $(a_i)_{i\in I}$ famille de nombres positifs.

\begin{definition}
	$\sum_{i \in I} a_i := \underset{F \subset I \mathrm{fini}}{\sup}\sum_{i \in F}a_i$
\end{definition}
\begin{propriete}
	Si $\sum_{i \in I} a_i$ est fini, alors $\{i \in I, a_i\neq 0\}$ est au plus dénombrable.
\end{propriete}

\begin{demo}
	$\{i\in I, a_i \in \mathbb R \setminus \{0\}\} \subset \underset{k \in \mathbb N}{\bigcup}
	\underset{\# \le k\sum_{i\in I} a_i}{\underbrace{\{i \in I, a_i \ge \frac{1}{k}\}}}$
\end{demo}

À partir de maintenant, on considérera $I$ dénombrable.

\begin{propriete}
	Si $\sigma : \mathbb N \rightarrow I$ est une bijection, alors
	$\sum_{i\in I}a_i = \underset{n \rightarrow +\infty}{\lim}\sum_{k=1}^{n}a_{\sigma (k)} =: \sum_{k=1}^{+\infty}a_{\sigma(k)}$
\end{propriete}

\begin{demo}
	$\forall F \subset I$ fini, $\sigma ^{-1}(F)$ est fini donc majoré par un entier $N$.
	
	$\sum_{i\in F}a_i = \sum_{k\in \sigma^{-1}(F)}a_{\sigma(k)} \le \sum_{k=1}^N a_{\sigma(k)} \le \sum_{k=1}^{+\infty}a_{\sigma(k)}$
	
	Donc par passage au sup : $\sum_{i\in I} a_i \le \sum_{k=1}^{+\infty}a_{\sigma(k)}$.
	
	Réciproquement, $\sum_{k=1}^N a_{\sigma(k)} = \sum_{i\in\sigma(\llbracket 1,N \rrbracket)} a_i \le \sum_{i\in I}a_i$. On conclut par passage à la limite.
\end{demo}

\begin{corollaire}
	Si $(a_k) \in \mathbb R_+^{\mathbb N}, \sum_{k=1}^{+\infty}a_k=\sum_{k=1}^{+\infty}a_{\sigma(k)}$ et ce quel que soit $\sigma : \mathbb N \rightarrow \mathbb N$ bijection.
\end{corollaire}

En particulier dans le cas $I=\mathbb N^2, (a_{i,j})_{(i,j)\in I}\in \mathbb R_+^I$ :
\begin{propriete}
	$\sum_{(i,j)\in I}a_{i,j} = \sum_{i=1}^{+\infty}\left(\sum_{j=1}^{+\infty}a_{i,j}\right) =
	\sum_{j=1}^{+\infty}\left(\sum_{i=1}^{+\infty}a_{i,j}\right)$
\end{propriete}

\begin{demo}
	$F\subset I$ fini. Il existe $N \in \mathbb N$ tel que $F \subset \llbracket 1,N\rrbracket^2$. Donc $\sum_{(i,j)\in F}a_{i,j} \le \sum_{i=1}^{N}\sum_{j=1}^{N}a_{i,j} \le
	\sum_{i=1}^N\sum_{j=1}^{+\infty}a_{i,j} \le
	\sum_{i=1}^{+\infty}\sum_{j=1}^{+\infty}a_{i,j}$.
	
	Réciproquement, $\forall N\in\mathbb N, \forall M \in\mathbb N, \sum_{i=1}^N\sum_{j=1}^Ma_{i,j} \le
	\sum_{(i,j)\in \mathbb N^2}a_{i,j}$.
	
	Donc $(M\rightarrow +\infty)$, $\sum_{i=1}^N\sum_{j=1}^{+\infty}a_{i,j} \le
	\sum_{(i,j)\in \mathbb N^2}a_{i,j}$.
	
	Donc $(N\rightarrow +\infty)$, $\sum_{i=1}^{+\infty}\sum_{j=1}^{+\infty}a_{i,j} \le
	\sum_{(i,j)\in \mathbb N^2}a_{i,j}$.
\end{demo}

\part*{Séries absolument convergentes}

Soit $(a_i)_{i\in I}$ une famille de réels tels que $\sum_{i\in I}|a_i|$ soit finie.

On définit $a_i^+ := \max(a_i,0)$, $a_i^- := \max(-a_i, 0)$.

Donc $a_i^+ - a_i^- = a_i$ et $a_i^+ + a_i^- = |a_i|$.

\begin{propriete}
	$\sum_{i\in I}a_i^+ - \sum_{i\in I}a_i^- = \sum_{k=1}^{+\infty}a_{\sigma(k)}$ et ce quel que soit $\sigma : \mathbb N \rightarrow I$ bijection.
\end{propriete}

\begin{demo}
	$\sum_{i\in I}a_i^+ \le \sum_{i\in I} |a_i|$ donc la somme est finie. Idem pour $\sum_{i\in I}a_i^-$.
	
	$\sum_{k=1}^n a_{\sigma(k)} = \sum_{k=1}^n a_{\sigma(k)}^+ - \sum_{k=1}^n a_{\sigma(k)}^- \underset{n\rightarrow +\infty}{\rightarrow}
	\sum_{k=1}^{+\infty}a_{\sigma(k)}^+ - \sum_{k=1}^{+\infty}a_{\sigma(k)}^-$
\end{demo}
\begin{corollaire}
	Sous réserve de convergence absolue, on a :
	
	\[\sum_{k=1}^{+\infty}a_k = \sum_{k=1}^{+\infty} a_{\sigma(k)}\]
	
	\[\sum_{i=1}^{+\infty}\sum_{j=1}^{+\infty}a_{i,j} =
	\sum_{j=1}^{+\infty}\sum_{i=1}^{+\infty}a_{i,j}\]
\end{corollaire}

\part*{Vocabulaire}

\begin{definition}
	Soit $X$ un ensemble. On dit que $\mathcal A \subset \mathcal P(X)$ est :
	\begin{itemize}
		\item une algèbre (d'ensembles) si elle est stable par union finie, intersection finie et passage au complémentaire, contient $\emptyset$ et $X$.
		\item une tribu (ou $\sigma$-algèbre) si c'est une algèbre stable par réunion/intersection dénombrable.
	\end{itemize}
\end{definition}

\begin{exemple}
	\begin{itemize}
		\item $\mathcal P(X)$ est une tribu.
		\item $\{\emptyset, X\}$ est une tribu.
	\end{itemize}
\end{exemple}

Si on se donne une partition finie de $X$ : $X=X_1 \sqcup X_2 \dots \sqcup X_k$, alors l'ensemble des $A \subset X$ de la forme $A=\underset{n\in I \subset \llbracket 1,k \rrbracket}{\bigcup}X_n$ est une tribu finie.

\begin{lemme}
	Toute algèbre finie est associée à une partition finie.
\end{lemme}

\begin{demo}
	Soit $\mathcal A$ une algèbre finie.
	
	$\forall x \in X, A(x) := \underset{x\in A}{\underset{A\in \mathcal A}{\bigcap}} A$.
	
	Pour $x$ et $y$ donnés, soit $A(x) = A(y)$, soit $A(x) \cap A(y) = \emptyset$.
	
	Fixons $x\in X, B\in\mathcal A$.
	\begin{itemize}
		\item Soit $x\in B$ et alors $A(x)\subset B$.
		\item Soit $x\in \comp B$ et alors $A(x)\subset \comp B$ i.e. $A(x)\cap B = \emptyset$
	\end{itemize}
	On conclut avec $B=A(y)$.
\end{demo}

\begin{definition}
	Si $\mathcal A$ est une algèbre de $X$ et $m:\mathcal A \rightarrow [0,+\infty]$ une fonction.
	
	On dit que $m$ est une \emph{mesure additive} si :
	\begin{itemize}
		\item $m(\emptyset) = 0$
		\item $m(A\sqcup B) = m(A) + m(B) \qquad (A\cap B = \emptyset)$
	\end{itemize}
\end{definition}

\begin{definition}
	Si $\mathcal T \subset \mathcal P(X)$ est une tribu, $m:\mathcal T \rightarrow [0,+\infty]$ est une \emph{mesure} si :
	\begin{itemize}
		\item $m(\emptyset) = 0$
		\item $m(\underset{i\in I}{\bigsqcup}A_i) = \sum_{i\in I}m(A_i)$ pour $(A_i)_{i\in I}$ famille dénombrable disjointe.
	\end{itemize}
\end{definition}
\begin{rem}
	Toute mesure est une mesure additive.
\end{rem}

\begin{rem}
On appelle parfois les mesures "mesures $\sigma$-additives".
\end{rem}

\begin{rem}
	Lorsque $m:\mathcal A \rightarrow [0,+\infty]$ est une mesure additive sur une algèbre, les propriétés suivantes sont équivalentes :
	\begin{enumerate}
		\item Si $A_i\in\mathcal A$ sont disjoints, $(A_i)$ dénombrable, $\underset{i\in I}{\bigsqcup}A_i \in \mathcal A$, alors $m(\underset{i\in I}{\bigsqcup}A_i) = \sum_{i\in I}m(A_i)$
		\item Si $A,A_i\in\mathcal A$, $A\subset \underset{i\in I}{\bigcup}A_i$, alors $m(A) \le \sum_{i\in I}m(A_i)$.
	\end{enumerate}
Dans ce cas, on dit que $m$ est $\sigma$-additive.
\end{rem}

\begin{demo}
	$(1) \Rightarrow (2)$ :
	
	Soit $A_i \in \mathcal A$. On définit $\tilde{A_i}$ par : $\tilde A_1 = A_1, \dots \tilde A_n = A_n\setminus \tilde A_{n-1} \quad \forall n \ge 1$
	
	Alors $\bigcup A_i = \bigsqcup \tilde A_i$.
	
	Si $A \subset \bigcup A_i$, alors $A \subset \bigsqcup \tilde A_i$. Alors $A=\bigsqcup(A\cap\tilde A_i)$.
	
	Donc $m(A)=m(\bigsqcup (\tilde A_i \cap A)) \le \sum m(A_i)$.

	$(2) \Rightarrow (1)$ :
	
	Si $A=\bigsqcup A_i \overset{(2)}{\Rightarrow} m(A) \le \sum m(A_i)$.
	
	$A\supset\overunderset{n}{i=1}{\bigsqcup}A_i$ quel que soit $n$.
	
	Donc $m(A) \ge \sum_{i=1}^n m(A_i)$. Donc ($n \rightarrow +\infty$), $m(A) \ge \sum_{i=1}^{+\infty}m(A_i)$.
	
\end{demo}

\begin{definition}
	Soit $f:\Omega \rightarrow X$ une application. Si $\mathcal A$ est une algèbre (ou une tribu) sur $\Omega$, alors on définit l'algèbre (tribu) image par :
	
\[ f_*\mathcal A = \{A\subset X, f^{-1}(A)\in \mathcal A\}\]
	Si $\mathcal A$ est une algèbre (tribu) sur $X$, alors \[f^*\mathcal A = \{f^{-1}(A), A\in\mathcal A\}\] est une algèbre (tribu) sur $\Omega$.
\end{definition}

La vérification du fait que $f^*\mathcal A$ et $f_*\mathcal A$ est une algèbre (tribu) découle des propriétés des préimages :

\begin{align*}
	&f^{-1}(A\cap B) = f^{-1}(A)\cap f^{-1}(B) \\
	&f^{-1}(A \cup B) = f^{-1}(A) \cup f^{-1}(B) \\
	&f^{-1}(A \setminus B) = f^{-1}(A) \setminus f^{-1}(B) 
\end{align*}

\begin{definition}
	Si $f:(\Omega, \mathcal A, m) \rightarrow X$ est une application, on définit la mesure image (ou la loi) comme la mesure :
	\[ (f_*m)(Y) := m(f^{-1}(Y))\] définie sur $f_*\mathcal A$.
\end{definition}

\begin{definition}
	$m$ est dite finie ssi $m(X) < +\infty$
\end{definition}
\begin{definition}
	$m$ est dite de probabilité ssi $m(X)=1$
\end{definition}

\begin{definition}
	$f:(\Omega,\tau) \rightarrow (X,\mathcal T)$ est dite mesurable si : \[\forall Y \in \mathcal T, f^{-1}(Y) \in \tau\] i.e.
	\begin{align*}
		&f_*\tau \supset \mathcal T \\
		&f^*\mathcal T \subset \tau
	\end{align*}
\end{definition}

\begin{exo}
	Soit $\Omega, X$ des ensembles, $\mathcal T$ une tribu sur $X$. Soit $f:\Omega \rightarrow X$ une application, $g:\Omega\rightarrow X$ une application à valeurs dans un ensemble fini $Y$. Alors $g$ est $f^*\mathcal T$ mesurable ssi $\exists h:(X,\mathcal T) \rightarrow (Y,\mathcal P(Y))$ mesurable telle que $g=h \circ f$. i.e. "$g$ est $f$-mesurable ssi $g$ ne dépend que de $f$".
\end{exo}

\part*{Modélisation d'une expérience aléatoire finie (ex : jets de dés)}

Soit $Y$ un ensemble fini représentant les issues possibles. Il y a 2 manières de représenter un tirage aléatoire sur $Y$.
\begin{enumerate}
	\item On se donne une mesure de probabilité sur $(Y,\mathcal P(Y))$. Pour ceci, il suffit de donner $p:Y\rightarrow[0,1]$ tel que $\sum_{y\in Y}p(y) = 1$. On note $P$ la mesure de probabilité ainsi créée.
	\item On se donne un espace de probabilité abstrait $(\Omega,\mathcal T, \mathbb P)$ et une application mesurable $f:\Omega \rightarrow Y$ telle que $f_*\mathbb P=P$.
\end{enumerate}
Pour passer de 1. à 2., il suffit de prendre $\Omega = Y$, $\mathcal T = \mathcal P(Y)$, $\mathbb P = P$, $f=\mathrm {id}$.

L'expérience aléatoire consistant à jeter un nombre fini $k$ de dés de valeurs possibles $Y_1, \dots, Y_k$ est simplement une expérience aléatoire à valeurs dans le produit $Y=Y_1 \times Y_2 \times \dots\times Y_k$.

La description en termes de variables aléatoires consiste donc à se donner une application mesurable $f : (\Omega, \mathcal T, P) \rightarrow Y$, c'est à dire $k$ applications mesurables $f_i : (\Omega, \mathcal T, P) \rightarrow Y_i$, définies sur \emph{le même espace de probabilités}.

\begin{definition}
	La loi de $f$ (qui est une probabilité sur $Y$) est dite \emph{loi jointe}. Les lois des $f_i$ (qui sont des probabilités sur $Y_i$) sont dites \emph{lois marginales}.
\end{definition}

\begin{rem}
	La loi jointe détermine les lois marginales, qui peuvent se décrire explicitement par $m_i(y_i)=\underset{y_1,\dots,y_{i-1},y_{i+1},\dots,y_k}{\sum} m(y_1,\dots,y_k)$.
	
	Plus abstraitement, ce soint les mesures images $m_i=(\Pi_i)_*m_i$ où $\Pi_i : Y \rightarrow Y_i$ est la projection.
\end{rem}

\begin{rem}
	La loi jointe est déterminée par $|Y_1|\times\dots\times |Y_k| - 1$ nombres réels ($-1$ à cause de la contrainte $\sum p = 1$).
	
	Les lois maginales sont déterminées par $|Y_1| + \dots + |Y_k| - k$ nombres réels, ce qui est beaucoup moins.
\end{rem}

Si on se donnes les marginales $m_1, \dots, m_k$, ilm existe de nombreuses lois jointes qui engendrent ces marginales. L'une d'entre elles est particulièrement intéressante : la loi produit $m((y1_,\dots,y_k))=m_1(y_1) \cdot \dots \cdot m_k(y_k)$, qui correspond (par définition) à des expériences indépendantes.

\begin{definition}
	\begin{itemize}
		\item Les événement $A,B$ dans un espace de probabilité $(\Omega,\mathcal T, P)$ sont dits indépendants si $P(A\cap B) = P(A)P(B)$.
		\item Si $(X_i, \mathcal T_i)_{1\le i\le k}$ sont des espaces mesurables (c'est à dire munis de tribus $\mathcal T_i$), les variables aléatoires (applications mesurables) $f_i : (\Omega,\mathcal T, P) \rightarrow (X_i,\mathcal T_i)$ sont dites \emph{indépendantes} si $\forall Z_i \in \mathcal T_i, P(f_1\in Z_1, \dots, P_k \in Z_k) = P(f_1 \in Z_i) \cdot \dots \cdot P(f_k \in Z_k)$
	\end{itemize}
\end{definition}

\begin{propriete}
	Les événements $A$ et $B$ sont indépendants ssi les variables aléatoires $\mathds{1}_A, \mathds{1}_B : (\Omega, \mathcal T, P) \rightarrow \{0,1\}$ le sont.
\end{propriete}

\begin{demo}
	Il suffit de montrer que $\comp A$ et $B$ sont indépendants (le reste est évident ou vient par symétrie).
	\begin{align*}
		P(\comp A\cap B) &= P((\Omega \setminus A) \cap B) \\
		&= P(B \setminus A\cap B) \\
		&= P(B) - P(A\cap B) \\
		&= P(B) - P(A)P(B) \\
		&= (1-P(A))P(B) \\
		&= P(\comp A)P(B)
	\end{align*} 
\end{demo}

\begin{definition}
	Les événements $A_1,\dots,A_k$ sont dits indépendants si $\mathds 1_{A_1} \dots \mathds 1_{A_k} : \Omega \rightarrow \{0,1\}$ le sont.
\end{definition}

\begin{rem}
	Il ne suffit pas d'avoir l'indépendance deux à deux ou $P(A_1 \cap \dots \cap A_k) = P(A_1) \cdot \dots \cdot P(A_k)$.
\end{rem}

\begin{propriete}
	Il suffit d'avoir $P(A_{i_1} \cap \dots \cap A_{i_k}) = P(A_{i_1}) \cdot \dots \cdot P(A_{i_k})$, et ce $\forall \{i_1, \dots, i_k\} \subset \llbracket 1,k \rrbracket$.
\end{propriete}

\begin{demo}
	Il faut montrer que
	\[ (*) \quad P(B_1 \cap \dots \cap B_k) = P(B_1) \cdot \dots \cdot P(B_k) \forall B_i \in \{\emptyset, A_i, \comp{A_i}, \Omega\}	\]
	Il découle de l'hypothèse que c'est vrai pour $B_i \in \{ \emptyset, A_i, \Omega\}$.
	
	Il suffit donc de constater que $(*)$ implique $P (\comp{B_1} \cap B_2 \cap \dots \cap B_k) = P(\comp{B_1})P(B_2) \cdot \dots \cdot P(B_k)$, ce qui se montre comme ce-dessus. On conclut par récurrence finie.
\end{demo}

\begin{exemple}
	Tirage non indépendant :
	
	On tire $-$ chiffres dans $\llbracket 1,6 \rrbracket$, en leur imposant d'être distincts. La loi jointe est donc : $P(y_1, \dots, y_6) = \begin{cases}
		0 &\mathrm{\ si\  non\  distincts} \\
		\frac{1}{6!} &\mathrm{\ si\ distincts}
	\end{cases}$.

	Les lois marginales sont : $P_1(y_1):= \underset{y_2,\dots,y_k}{\sum} P(y_1,\dots,y_k) = \frac{5!}{6!} = \frac{1}{6}$. Les lois marginales sont donc les mêmes que pour un tirage indépendant !
\end{exemple}
\begin{definition}
	On dit que $f_i,\ i\in I$ sont indépendantes si $f_i,\ i\in F$ le sont pour tout $F\subset I$ fini.
\end{definition}

\part*{Modélisation d'une suite infinie de jets dés indépendants}

Donnons-nous une suite infinie d'espaces de probabilités finis $(Y_i,P_i)$ (la tribu est $\mathcal P(Y_i)$).

Pour chaque $n$, on a vu que l'on peut trouver des variables aléatoires indépendantes $f_i:(\Omega_n,\mathcal T_n, P_n) \rightarrow Y_i$ de loi $P_i$.

Question : peut-on prendre $(\Omega_n,\mathcal T_n, P_n)$ indépendant de $n$?

\begin{theorem}
	Il existe un espace de probabilité $(\Omega,\mathcal T, P)$ et une suite de variables aléatoires $f_i : \Omega \rightarrow Y_i$ qui sont indépendantes et de loi $P_i$.
\end{theorem}

\begin{rem}
	Les variables aléatoires $f_i$, $i\in \mathbb N$ sont indépendantes ssi $f_1, \dots, f_n$ le sont pour tout $n$.
\end{rem}

L'hypothèse d'indépendance consiste donc à dire que, pour tout $n$ et pour tout $(y_1, \dots, y_n) \in Y_1 \times \dots \times Y_n$, l'événement $\{f_1=y_1,\dots,f_n=y_n\}$ est mesurable ()$\in\mathcal T$) et de mesure $P(f_1=y_1, \dots f_n=y_n) = P_1(y_1)\cdot\dots\cdot P_n(y_n)$.

En termes de loi, ceci implique que $\{y_1\}\times \dots \{y_n\} \times Y_{n+1} \times \dots$ est mesurable sur $X:=\prod Y_i$ et que sa mesure est $m(\{y_1\}\times \dots \times Y_{n+1}) = P_1(y_1) \cdot \dots \cdot P_n(y_n)$.

\section*{Introduction de l'algèbre $\mathcal A_\infty$ engendrée par les cylindres finis}

Sur le produit $X = \prod Y_i$, pour $n$ fixé, les ensembles de la forme $\{y_1\}\times \dots \times \{y_n\}\times Y_{n+1} \times \dots$ forment une partition finie (ce sont les cylindres finis), qui engendre une algèbre finie $\mathcal A_n$ (qui est donc aussi une tribu).

C'est l'algèbre engendrée par les $n$ premières coordonnées. En effet si $\Pi : X \rightarrow Y_1 \times \dots \times Y_n$ est la projection, alors $\mathcal A_n = \Pi^* (\mathcal P(Y_1 \times \dots \times Y_n))$.

Cette algèbre décrit les parties de $X$ qui peuvent être décrites en termes des $n$ premières coordonnées.

On a $\mathcal A_n \subset \mathcal A_{n+1}$. On note $\mathcal A_\infty = \underset{n\ge 1}{\bigcup} \mathcal A_n$.

$\mathcal A_\infty$ est donc l'algèbre des parties de $X$ qui dépendent d'un nombre fini de coordonnées. C'est l'algèbre engendrée par les cylindres finis.

Contrairement aux $\mathcal A_n$, $\mathcal A_\infty$ est infinie et ce n'est pas une tribu !

L'hypothèse d'indépendance des $f_i$ implique que la loi $m$ doit être définie sur $\mathcal A_\infty$, et qu'elle y est déterminée par la relation \[ (*) \quad m(\{y_1\}\times \dots \times \{y_n\} \times Y_{n+1} \times \dots ) = P_1(y_1) \cdot\dots\cdot P(y_n)\]

\begin{theorem}
	Il existe sur $X = \prod Y_i$ une tribu $\tau$, qui contient $\mathcal A_\infty$, et une mesure $m$ sur $\mathcal T$ qui vérifie $(*)$.
\end{theorem}

On vient en fait de voir que le théorème 1. implique le théorème 2.
Réciproquement, il suffit de prendre $\Omega = X, \mathcal T = \tau, P=m, f=\text{projection}$.

Pour démontrer l'utilité du théorème 2., donnons des exemples d'ensembles qu'il est naturel de considérer et qui sont dans $\tau$ mais pas dans $\mathcal A_\infty$. On suppose $Y_i \subset \mathbb R$

\begin{exemple}
	L'ensemble $\{(y_i)\in X, \frac{y_1 + \dots y_n}{n} \rightarrow l\}$ est mesurable. En effet, il s'écrit : $\underset{k\ge 1}{\bigcap}\ \underset{n\in \mathbb N}{\bigcup}\ \underset{m \ge n}{\bigcap} \{\left| \frac{y_1+ \dots y_n}{n} - l \right| \le \frac{1}{k}\}, \text{ i.e. } \forall k \ge 1, \exists n\in \mathbb N, \forall m\ge n, \dots$. 
	Chacun des ensembles est dans $\mathcal A_\infty$ donc l'ensemble considéré est dans $\tau$.
\end{exemple}

\section*{Quelques résultats d'extension des mesures}
	Si $A_n$ est une suite d'ensembles, on note :
	\[ \liminf A_n = \underset{n}{\bigcup}\ \underset{m\ge n}{\bigcap} A_m = \{A_i \text{ APCR}\}\]
	\[ \limsup A_n = \underset{n}{\bigcap}\ \underset{m\ge n}{\bigcup} A_m = \{A_i \text{ infinitely often (i.o.)}\}\]
	
	Si $\tau$ est une tribu, que les $A_n \in \tau$, alors $\limsup A_n \in \tau$ et $\liminf A_n \in \tau$.
	
	\begin{propriete}
		\begin{itemize}
			\item $\liminf A_n \subset \limsup A_n$
			\item $\liminf \comp{A_n}= \comp{(\limsup A_n)}$
		\end{itemize}
	\end{propriete}
	\begin{demo}
		$\forall m,M, \quad \underset{n\ge m}{\bigcap} A_n \subset \underset{n \ge M} A_n$.
		Donc $\underset{n\ge m}{\bigcap} A_n \subset \limsup A_n$, donc $\liminf A_n \subset limsup A_n$
	\end{demo}
\begin{exo}
	$\limsup \mathds 1_{A_n} = \mathds 1_{\limsup A_n}$
\end{exo}

\begin{exemple}
	On considère un tirage aléatoire indépendant $f_n \in {-1, 1}^{\mathbb N}$, ce que l'on voit comme un jeu de hasard (le joueur gagne ou perd $1$ à chaque étape). Étant donnée la richesse initiale $r_0$ et un objectif $R$, on considère l'événement \{le joueur atteint la richesse $R$ avant de se ruiner\}.
	
	Il s'écrit $\underset{n \ge 1}{\bigcup} \{y_1 + \dots y_n \ge -r_0 \quad \forall k < n \text{ et } y_1 + \dots + y_k = R-r_0\}$.
	
	C'est une réunion dénombrable d'éléments de $\mathcal A_\infty$
\end{exemple}

Le théorème 2 sera déductible du théorème suivant :
\begin{theorem}[Hahn-Kolmogorov]
	Soit $\mathcal A$ une algèbre d'ensembles sur $X$. Soit $\underline m$ une mesure de probabilité additive sur $\mathcal A$, qui vérifie la propriété de $\sigma$-additivité.
	
	Alors il existe une tribu $\tau$ contenant $\mathcal A$, et une mesure de proba $m$ sur $\tau$ qui prolonge $\underline m$. De plus, on peut prendre : $m(B) = \underset{B \subset \bigcup A_i} \inf \  \underset{i \in \mathbb N}{\sum} \underline m (A_i)$, où le $\inf$ est pris sur les recouvrements dénombrables de $B$ par des éléments de $\mathcal A$.
\end{theorem}

Pour démontrer le théorème 2, on va appliquer le théorème 3 avec $\mathcal A = \mathcal A_\infty$, et $\underline m$ la mesure additive déterminée par $\underline m (\{y_1\}\times \dots \times \{y_n\} \times Y_{n+1} \times \dots) = P_1(y_1) \dots P_n(y_n)$.

Il nous suffit donc de vérifier que cette mesure additive a la propriété de $\sigma$-additivité.

\begin{propriete}
	Toute mesure additive sur $\mathcal A_\infty$ est $\sigma$-additive.
\end{propriete}

\begin{demo}
	Soient $A \in \mathcal A_\infty$ et $A_i \in \mathcal A_infty$ tel que $A \subset \underset i \bigcup A_i$, alors $\exists n, A \subset \bigcup_{i=1}^n A_i$.
	
	\begin{itemize}
		\item méthode savante : c'est la compacité de $A$ dans $X$ muni de la topologie produit (les $A_i$ sont ouverts et compacts)
		\item à la main :
		On pose $B_n = A \setminus \bigcup_{i=1}^n A_n$. On veut montrer que $\exists n, B_n = \emptyset$, sachant que $\underset {n\ge 0}\bigcap B_n = \emptyset$.
		
		On suppose que $B_n \ne \emptyset, \forall n$. On note $B_n(y_1) := \Pi_1^{-1}(y_1) \cup B_n$, ce sont les éléments de $B_n$ qi commencent par $y_1$.
		
		Pour chaque $y_1$, $n \mapsto B_n(y_1)$ est décroissante. Comme $B_n = \underset {y_1 \in Y_1} \bigcup B_n(y_1)$ (union finie) (et $B_n \ne \emptyset$), il existe $y_1$ tel que les $B_n(y_1)$ sont tous non vides.
		
		On fixe maintenant un tel $y_1$ et on reprend le même raisonnement sur $y_2$, puis... On obtient de la sorte une suite $y$.
		
		Ainsi, il existe une suite $(y_1, \dots) \in B_n \forall n$ car $\forall n, \exists k_n, B_n\in\mathcal A_{k_n}$.
		
		Ainsi, $\forall n, B_n \ni y$ donc $\bigcap B_n \ne \emptyset$. Absurde.
	\end{itemize}
\end{demo}

\begin{propriete}
	Dans le contexte du théorème d'Hahn-Kolmogorov, $m^* : \mathcal P(X) \rightarrow [0,\infty]$ est une \emph{mesure extérieure}, c'est à dire que $m^*(\emptyset) = 0$, $m^*$ est croissante, et $m^*\left(\underset{i\in \mathbb N} \bigcup Z_i\right) \le \underset{i\in\mathbb N}\sum m^*(Z_i), \forall Z_i$.
\end{propriete}

\begin{demo}
	Démontrons la dernière propriété. Fixons $\varepsilon > 0$. Pour tout $i$, il existe un recouvrement $A_{i,j}, j\in \mathbb N$ de $Z_i$ tel que $\underset j \sum \underline m(A_{i,j}) \ge m^*(Z_i) \ge \underset j \sum \underline m (A_{i,j}) - \varepsilon 2^{-i}$, alors $A_{i,j}, i\in \mathbb N, j \in \mathbb N$ est un recouvrement de $\bigcup Z_i$, et $m^*(\bigcup Z_i) \le \underset {i,j} \sum \underline m (A_{i,j}) \le \underset {i \ge 1} \sum (m^*(Z_i) + \varepsilon 2^{-1}) \le \varepsilon + \underset {i \ge 1} \sum m^*(Z_i)$.
\end{demo}

\begin{demo}[Démonstration du théorème d'Hahn-Kolmogorov]
	Deux étapes : \begin{enumerate}
		\item $m^*|_{\mathcal A} = \underline m$
		Si $A \subset \underset i \bigcup A_i$, alors $\underline m (A) \le \sum \underline m (A_i)$ par $\sigma$-additivité de $\underline m$. En prenant l'inf, on obtient $\underline m(A) \le m^*(A)$. L'inégalité réciproque s'obtient en considérant le recouvrement trivial $A_1 = A, A_2=A_3 = \dots = \emptyset$.
		\item On dit que $Y \subset X$ est mesurable si, pour tout $\varepsilon > 0, \exists A \in \mathcal A$ tel que $m^*(Y \Delta A) \le \varepsilon$. Alors l'ensembre $\mathcal T$ des parties mesurables est une algèbre.
		\begin{demo}
			\begin{itemize}
				\item si $m^*(Y \Delta A) \le \varepsilon$, alors $m^*(\comp Y \cap \comp A) \le \varepsilon$, donc $\mathcal T$ est stable par complément.
				\item Soient $Y, Z$ mesurables et $A, B$ tels que $m^*(Y \Delta A) \le \varepsilon, m^*(Z \Delta B) \le \varepsilon$ alors $m^*((Y \cup Z) \Delta (A \cup B)) \le 2\varepsilon$ car $(Y \cup Z)  \Delta (A \cup B) \subset (Y \Delta A) \cup (Z \Delta B)$.
			\end{itemize}
		\end{demo}
		\item $m^*$ est une mesure additive sur $\mathcal T$.
		\begin{demo}
			$Y, Z$ disjoints, $A,B$ comme ci-dessus. 
			
			$(A\cap B) = (Y \cup (A \setminus Y)) \cap (Z \cup (B \setminus Z)) \subset Y \cap Z \cup (B \setminus Z) \cup (A \setminus Y)$ 
			
			donc $\underline m (A \cap B) \le 2\varepsilon$
			
			$A \cup B = (Y \cup (A \setminus Y)) \cup (Z \cup (B \setminus Z)) \subset Y \cup Z \cup (A \setminus Y) \cup (B \setminus Z)$
			
			$\underline m (A \cup B) \le m^*(Y \cup Z) +2\varepsilon$
			
			et $\underline m (A \cup B) = \underline m(A) + \underline m(B) - \underline m(A \cap B) \ge \underline m(A) + \underline m(B) -2\varepsilon \ge m^*(Y) - \varepsilon + m^*(Z) - \varepsilon - 2\varepsilon$.
			
			Finalement, $m^*(Y) + m^*(Z) \le m^*(Y \cup Z) + 6\varepsilon$ 
		\end{demo}
	Comme $m^*$ est une mesure extérieure et une mesure additive sur l'algèbre $\mathcal T$, elle a la propriété de $\sigma$-additivité.
	\item $\mathcal T$ est une tribu.
	\begin{demo}
		$Y_i \in \mathcal T$. On veut montrer que $Y_\infty := \underset i \bigcup Y_i \in \mathcal T$. On peut supposer que les $Y_i$ sont disjoints. Alors $\forall n, m^*(\underset{i = 1}{\overset{n} \bigsqcup} Y_i) = \underset{i = 1}{\overset{n} \sum} m^*(Y_i) \le m^*(X) = 1$. Donc la série $\sum m^*(Y_i)$ converge, donc $\forall \varepsilon, \exists n, \underset{i = n+1}{\overset{+\infty} \sum} m^*(Y_i) \le \varepsilon$.
					
		Alors en posant $Z= \underset{i = 1}{\overset{n} \bigcup} Y_i$, on a $m^*(Y_\infty \setminus Z) \le \varepsilon$, $Z \subset Y_\infty$. Ensuite, on prend $A \in \mathcal A$ tel que $m^*(A\setminus Z) \le \varepsilon, m^*(Z\setminus A) \le \varepsilon$. On obtient $A \setminus Y_\infty \subset A \setminus Z, Y_\infty \setminus A \subset (Z \setminus A) \cup (Y_\infty \setminus Z)$.
	\end{demo}
	\end{enumerate}
\end{demo}

\textbf{Complément :} on aurait pu donner une autre preuve du théorème 3 basée sur un résultat général sur les mesures extérieures. Lorsque $m^*$ est une mesure extérieure, on dit que $Y \subset X$ est $m^*$-mesurable si

$\forall Z \subset X, m^*(Z) = m^*(Z \cap Y) + m^*(Z \cap \comp Y)$.

\begin{theorem}[Carathéodory]
	Si $m^*$ est une mesure extérieure, l'ensemble $\mathcal T$ des parties $m^*$-mesurables est une tribu, et $m^*|_{\mathcal T}$ est une mesure.
\end{theorem}
\begin{rem}
	Dans le cas du théorème de Hahn, la tribu $\mathcal T$ est la même que celle introduite dans la démonstration précédente.
\end{rem}

\begin{demo}[Carathéodory $\Rightarrow$ Hahn-Kolmogorov]
	
	Il suffit de montrer que les éléments de $\mathcal A$ sont $m^*$-mesurables, et que $m^*|_{\mathcal A} = \underline m$.
	\begin{itemize}
		\item $m^*(A) \le \underline m(A) \forall A \in \mathcal A$
		\item $m^*(A) \ge \underline m (A) \forall A \in \mathcal A$. En effet, si $A \subset \underset i \bigcup A_i$, on peut supposer les $A_i$ disjoints. Alors par $\sigma$-additivité de $\underline m$ sur $\mathcal A$ : $\underline m(A) = \underset i \sum \underline m(A_i) \ge m^*(A)$.
		\item Soit $A \in \mathcal A$ et $Zin \mathcal P(X)$. On considère un recouvrement $A_i$ de $Z$.
		
		$\underset i \sum \underline m(A_i) = \underset i \sum \underline m (A_i \cap A) + \underline m(A_i \cap \comp A) \ge m^*(Z \cap A) + m^*(Z \cap \comp A)$.
		
		On prend l'inf : $m^*(Z) \ge m^*(Z \cap A) + m^*(Z \cap \comp A)$. L'autre inégalité découle de la sous-additivité.
	\end{itemize}
\end{demo}

\begin{demo}[Carathéodory]
	
	\begin{enumerate}
		\item $\mathcal T$ est une algèbre. 
		\begin{demo} On a $\emptyset \in \mathcal T, X \in \mathcal T$, et stabilité par complément de manière triviale.
		
		$A,B \in \mathcal T \Rightarrow \forall Y, m^*(Y) = m^*(Y\cap A) + m^*(Y \cap \comp A) = m^*(Y \cap A \cap B) + m^*(Y \cap A \cap \comp B) + m^*(Y \cap \comp A \cap \comp B) + m^*(Y \cap \comp A \cap B)$.
		
		\begin{rem}
			$\comp{(A \cap B)} = (\comp B \cap A) \cup (B \cap \comp A) \cup (\comp A \cap \comp B)$
		\end{rem}
		Donc $m^*(Y) \ge m^*(Y \cap (B \cup A)) + m^*(Y \cap \comp{(B \cap A)})$
		\end{demo}
		\item $m^*$ est additive sur $\mathcal T$
		\begin{demo}
			$A,B \in \mathcal T$, $A\cap B = \emptyset$.
			
			$m^*(A\cup B) = m^*((A\cup B)\cap A) + m^*((A \cup B) \cap \comp A) = m^*(A) + m^*(B)$
		\end{demo}
		\item $\mathcal T$ est une tribu.
		\begin{demo}
			soit $A_n$ une suite d'éléments deux à deux disjoints de $\mathcal T$. Posons $B_n =\bigcup_{k=1}^n A_n$ et $B_\infty = \bigcup_{k=1}^\infty A_n$.
			
			$\forall Y \subset X, m^*(Y \cap B_n) = m^*(Y \cap B_n \cap A_n) + m^*(Y \cap B_n \cap \comp{A_n}) = m^*(Y \cap A_n) + m^*(Y \cap B_{n-1})$.
			
			Donc $m^*(Y\cap B_n) = \sum_{k=1}^n m^*(Y \cap A_k)$.
			
			Alors $m^*(Y) = m^*(Y) = m^*(Y \cap B_n) + m^*(Y \cap \comp{B_n}) \ge \sum_{k=1}^n m^*(Y \cap A_k) + m^*(Y \cap \comp{B_{infty}})$.
			
			À la limite : $m^*(Y) \ge \sum_{n=1}^\infty m^*(Y \cap A_n) + m^*(Y \cap \comp{B_{\infty}})$
		\end{demo}
	\end{enumerate}
\end{demo}

On peut cependant se poser la question de l'unicité de $m^*$ dans Hahn-Kolmogorov.

\begin{theorem}
	Si $\mu : B \rightarrow [0,1]$ est une mesure sur une tribu $B \subset \A$, $\mu|_\A = \underline m$, alors $\mu=m^*$ sur $\T \cap B$.
\end{theorem}

\begin{rem}
	Il existe une plus petite tribu contenant $\A$ ($\underset{\T \subset \A,\ \T \text{ tribu}}{\bigcap} \T$). Sur cette tribu, il existe une unique mesure prolongeant $\underline m$.
\end{rem}

\begin{demo}
	\begin{enumerate}
		\item Si $B \subset \underset i \bigcup A_i, A_i \in \A_i, B \in \B$, alors $\mu(B) \le \underset i \sum \mu(A_i) = \underset i \sum \underline m(A_i)$. En prenant l'inf sur les familles $A_i$, on conclut $\mu \le m^*|_\B$
		\item Comme $\mu(B) \le 1- \mu(\comp B)$, si $B \in \T$, on a $A-m^*(\comp B) = m^*(B)$ donc $\mu(b) \ge m^*(B)$ et donc $\mu(B) = m^*(B) si B\in \T$
	
	\end{enumerate}
\end{demo}

\section*{Loi des grands nombres}

On se donne $Y \subset \R$ fini, une mesure de probabilité $p$ sur $Y$, et une suite finie $f_{i,i\in\N} : \Omega \rightarrow Y$ de variables aléatoires iid suivant la loi $p$. L'existence d'une telle suite découle des théorèmes de la section précédente.

\begin{definition}
	Si $f : \Omega \rightarrow \R$ est une variable aléatoire prenant un nombre fini de valeurs, on note $E(f) = \underset{y \in f(\Omega)} \sum yP(f=y)$ l'espérance de $f$.
\end{definition}

Dans notre contexte on note $e := E(f)$.

On définit $S_n = \frac{f_1 + \dots + f_n} n : \Omega \rightarrow \R$. Chacune des variables aléatoires $S_n$ prend un nombre fini de valeurs, mais les variables $S_n$ ne sont pas indépendantes.

On veut montrer les trois énoncés suivants :

\begin{theorem}[Loi faible des grands nombres]
	\[P\left(\left| \frac{S_n} n -e \right| \ge \eps\right) \rightarrow 0\]
\end{theorem}

\begin{theorem}[Loi forte des grands nombres]
	\[P \left(\frac {S_n} n \rightarrow e\right) = 1\]
\end{theorem}

\begin{theorem}
	\[\forall \alpha > \frac 1 2, \quad P\left(\frac {S_n - ne} {n^\alpha} \rightarrow 0\right) = 1\]
\end{theorem}

\subsection*{Quelques outils de théorie de la probabilité}

Pour démontrer ces résultats, on va avoir besoin d'un certain nombre d'autres outils.

\begin{theorem}[Inégalité de Markov]
	Si $f$ est une variable aléatoire positive, 
	\[ \forall a \in \R^+_*, \quad P(f > a) \le \frac {E(f)} a \]
\end{theorem}

\begin{demo}
	On écrit la définition de $E(f)$, on coupe la somme en deux selon $y>a$ ou $y \le a$, on majore brutalement et on conclut.
\end{demo}

\begin{theorem}[Inégalité de Bienaymé-Tchebychev]
	\[ \forall a \in \R^+_*, \quad P(|f-E(f))| > a) \le \frac {\var(f)} {a^2} \]
\end{theorem}

\begin{demo}
	On élève l'événement au carré, on conclut par Markov.
\end{demo}

\begin{propriete}
	$E(XY) := E(X)E(Y) + \cov (X,Y)$,
	
	$\var(X+Y) = \var(X) + \var(Y) + 2\cov (X,Y)$
\end{propriete}

\begin{demo}
	Il suffit de l'écrire.
\end{demo}

\begin{lemme}
	Si $X$ et $Y$ sont deux variables aléatoires indépendantes, $\cov(X,Y) = 0$.
\end{lemme}
\begin{demo}
	Trivial.
\end{demo}

\begin{propriete}[Convergence monotone]
	Soit $(\Omega, \T, m)$ un espace mesuré.
	
	Si $(A_n)$ est une suite décroissante et que $m(A_1)$ est fini, alors $m(\underset {n\in\N} \bigcap A_n) = \lim m(A_n)$.
	
	Si $(A_n)$ est une suite croissante, alors $m(\underset {n\in\N} \bigcup A_n) = \lim m(A_n)$.
\end{propriete}

\begin{demo}
	On pose $B_n = A_n \setminus A_{n-1}$, et par double passage à la limite, la propriété sur les suites croissantes est immédiate. Le résultat sur les suites décroissantes vient du passage au complémentaire.
\end{demo}

\begin{lemme}[Fatou ensembliste]
	\begin{itemize}
		\item $m(\liminf A_n) \le \liminf m(A_n)$
		\item Si $m$ est finie, $m(\limsup A_n) \ge \limsup m(A_n)$
		\item Si $m$ est finie et $\limsup A_n = \liminf A_n = A$, alors $m(A_n) \rightarrow A$
	\end{itemize}
\end{lemme}

\begin{demo}
	On pose $B_n = \underset {m \ge n} \bigcap A_m$. C'est une suite croissante.
	
	$m(B_n) \rightarrow m(\underset n \bigcup B_n) = m(\liminf A_n)$
	
	$m(B_n) \le m(A_n)= \Rightarrow \liminf m(A_n) \le m(\liminf A_n)$
\end{demo}

\begin{lemme}[Premier lemme de Borel-Cantelli]
	Si $\underset {n\ge 0} \sum m(A_n)$ est finie, alors $m(\limsup A_n) = 0$.
\end{lemme}

\begin{demo}
	$B_n := \underset {m\ge n} A_m$.
	
	$m(B_n) \le \sum_{m \ge n}^\infty m(A_n) \rightarrow 0$ (reste de série convergente)
	
	Or, $\lim m(B_n) = m(\limsup A_n) = 0$.
\end{demo}

\begin{theorem}[Inégalité de Kolmogorov]
	$P( \underset {A \le k \ le n} \max |\tilde S_k| \ge a) \le \frac {n \var(f)} {a^2}$
\end{theorem}

\begin{demo}
	$T(\omega) :=$ le premier temps pour lequel $|\tilde S_n \ge a$. $T(\omega) \in \N \cup \{+\infty\}$
	
	$(T = k) = \{|\tilde S_1| < a \} \cap \dots \cap \{|\tilde S_{n-1}| < a \} \cap \{|\tilde S_n| \ge a \} \in \A_k$.
	
	$T$ est ainsi un \emph{temps d'arrêt}.
	
	$\begin{aligned}
		\var \tilde S_n &= E(\tilde S_n^2) \ge \sum_{k=1}^n E(S_n^2 \mathds 1_{\{T=k\}}) \\
		&= \sum_{k=1}^n E((\tilde S_n + \tilde S_k - \tilde S_k) \mathds 1_{\{T=k\}}) \\
		&= \sum_{k=1}^n E(\tilde S_k^2 \mathds 1_{\{T=k\}}) + \sum_{k=1}^n E((\tilde S_n - \tilde S_k)\tilde S_k \mathds 1_{\{T=k\}}) \\
		&\ge \sum_{k=1}^n a^2P(T=k) + 0 + 0 (*) \\
		&\ge a^2 P(M_n \ge a)
	\end{aligned}$
et $(*) : \tilde S_n - \tilde S_k = \tilde f_{k+1} + \dots + \tilde f_n$. Or $\tilde S_k \mathds 1_{\{T=k\}}$ ne dépend que des $k$ premières valeurs (indépendance).
\end{demo}

\begin{rem}
	Illustration de la notion de temps d'arrêt :
	
	On considère un jeu de hasard : une suite $f_i$ de v.a. i.i.d. à valeurs dans $\{-1, 1\}$, avec $P(1) =p$.
	
	Supposons que le joueur choisit un temps $T(\omega)$ pour miser. Peut-il optimiser sa probabilité de gain $P(f_{T(\omega)}(\omega) = 1)$ ?
	
	On peut choisir $T(\omega)$ le premier temps tel que $f_{T(\omega)} = 1$, mais cela nécessite de connaître tous les tirages.
	
	En réalité, on ne dispose pas de l'almanach des sports, on n'a que l'information des $k-1$ premiers tirages, i.e. $\{T=k\} \in \A_{k-1}$, c'est un temps d'arrêt.
	
	\begin{propriete}
		Si $T$ vérifie cette condition, $P(f_{T(\omega)}(\omega)=1) = p$.
	\end{propriete}
	\begin{demo}
		$P(f_{T(\omega)}(\omega)=1) = \sum_{k=1}^\infty P((T=k) \cap (f_k =1))$. On conclut par indépendance.
	\end{demo}
\end{rem}

\begin{theorem}[Inégalité de Hoeffding]
	\[ P\left( \left| \tilde S_n \right| \ge a \right) \le 2 \exp (-\frac{2a^2}{Cn}), \qquad C = (\max f - \min f)^2\]
\end{theorem}

\begin{lemme}
	Pour $\tilde f$ une v.a. centrée prenant un nombre fini de valeurs, on a : 
	\[ E\left( e^{\theta \tilde f} \right) \le e^{C\theta^2/8}, \qquad C=(\max \tilde f - \min \tilde f)^2\]
\end{lemme}

\begin{demo}[lemme]
	On pose $g(\theta):=\ln(E(e^{\theta\tilde f}))$. $g$ est $\mathcal C^\infty$.
	
	On a : $g(0) = 0$, $g'(\theta) = \frac{E(\tilde f e^{\theta\tilde f})}{E(e^{\theta \tilde f})}$ donc $g'(0) = E(\tilde f) = 0$.
	
	Au voisinage de $\theta = 0$, il existe une constance $c$ telle que $g(\theta) \le c\theta^2$.
	
	$g''(\theta) = \frac{E(\tilde f^2 e^{\theta \tilde f}) E(e^{\theta\tilde f}) - E(\tilde f e^{\theta\tilde f})^2} {E(\theta e^{\theta\tilde f})^2}$
	
	Ceci est la variance de la loi de proba sur $\tilde Y$ donnée par $P_\theta(\tilde y) = \frac{P(\tilde y)e^{\theta\tilde f}}{E(e^{\theta \tilde f})}$.
	
	En effet, $g''(\theta) = E \left(  \frac{\tilde f^2 e^{\theta\tilde f}} {E(e^{\theta \tilde f})} \right) - E \left( \tilde f \frac{e^{\theta\tilde f}}{E(e^{\theta\tilde f})} \right)^2$.
	
	Si $g$ est une v.a. prenant un nombre fini de valeurs, alors $\var(g) \le (\max g - \min g)^2/4$. On remarque que la variance est invariante à translation de $g$ près, donc on peut supposer $\max g = - \min g$. Or, $\var(g) = E(g^2) - E(g)^2 \le E(g^2) \le (\max g)^2 \le (\max g - \min g)^2/4$.
	
	Ainsi, par l'inégalité des accroissements finis, comme $g''(\theta) \le \frac C 4$, on a $g(\theta) \le \frac C 8 \theta^2$
	
	Donc $E(e^{\theta\tilde f}) \le \exp\left( \frac{C\theta^2} 8 \right)$
\end{demo}

\begin{demo}[Hoeffding]
	$P(\tilde S_n \ge a) = P(e^{theta\tilde S_n} \ge e^{\theta a}) \le e^{-a\theta}E(e^{\theta\tilde S_n})$ par Markov.
	
	$E(e^{\theta\tilde S_n}) = E(\prod e^{\theta\tilde f_i}) = \prod E(e^{\theta\tilde f_i}) = E(e^{\theta\tilde f})^n$.
	
	$P(\tilde S_n \ge a) \le e^{-a\theta} E(e^{\theta\tilde f})^n \le \exp \left(\frac {nC\theta^2} 8 - \theta a \right)$.
	
	On optimise par rapport à $\theta$ ($\theta = \frac{4a}{nC}$), et on conclut.
\end{demo}

\begin{rem}
	\textbf{Intérêt de ces inégalités en statistiques}
	
	Bienaymé-Tchebychev : $P(|\tilde S_n| \ge a) \le \frac{n\var(f)}{a^2}$
	
	Hoeffding : $P(|\tilde S_n| \ge a) \le 2\exp(-\frac{2a^2}{nC})$
	
	Dans les deux cas, on note une décroissance en $\frac n {a^2}$.

	Exemple d'application : sondage. La population peut avoir deux avis : $Y = \{ 0, 1\}$. On cherche à estimer par un sondage quelle est la proportion $p$ de la population qui a l'avis $1$ ($p=P(1)$).
	
	On interprète un sondage comme étant une suite finie de variable aléatoires iid $f_1, \dots, f_n$ tirées suivant la loi ci-dessus.
	
	On s'attend à ce que $\frac {S_n} n \approx p$. Les inégalités rappelées ci-dessus nous donnent des majorations de $P\left(\left|\frac{S_n} n - p \right| \ge \eps \right)$. Dans ce contexte, Hoeffding donne de meilleures estimations.
	
	Exemple de valeurs numériques :
	\begin{center}
		\begin{tabular}{|c|c||c|}
			\hline
			n & $\eps$ & Résultat \\
			\hline
			1000 & 5\% & $p=1,3$\% \\
			\hline
			1000 & 1\% & $n\eps^2 =1$, on ne peut rien conclure \\
			\hline
		\end{tabular}
	\end{center}

	La première ligne indique que la probabilité d'être à plus de 5\% d'erreur est d'au plus 1,3\%.
	
	\textbf{Intervalle de confiance :}
	
	Ici, on fixe $p$ la probabilité d'erreur, et on cherche $\eps$, c'est à dire, par Hoeffding, $\eps \le \sqrt{\frac 1 {2n} \ln\left(\frac 2 p \right)}$. Par exemple, pour $n=1000$, $p=5\%$, on obtient $\eps=4,"\%$.
	
	En général, avec ces données, on donne $\eps = 3\%$. Cette disparité vient de la non-optimalité de l'inégalité de Hoeffding, et par le fait qu'il existe des modèles plus précis (approcher cette binomiale par une gaussienne grâce au théorème central limite par exemple).
\end{rem}

\subsection*{Démonstrations des lois des grands nombres}

\begin{demo}[Loi faible des grands nombres]
	
	$P(\left|\frac {S_n} n - e \right| \ge \alpha) = P(|S_n - E(S_n)| \ge n\alpha) \le \frac{\var(S_n)}{n^2\alpha^2} = \frac{n \var(f)}{n^2\alpha^2} = \frac{\var(f)}{n\alpha^2}$
\end{demo}

\begin{demo}[Loi forte des grands nombres]
	$\sum_n P(A_{n^2}(\eps))$ converge.
	
	Donc $m(\limsup(A_{n^2}(\eps)))= 0$ d'après Borel-Cantelli. Donc $\frac {S_{n^2}} {n^2} \rightarrow E(f)$ p.p.
	
	Montrons alors que si $\frac {S_{n^2}} {n^2} \rightarrow E(f)$ alors $\frac {S_n} n \rightarrow E(f)$.
	
	On note $M = \max |f|$. Soit $k(n)$ tel que $k(n)^2 \le n < (k(n) + 1)^2$.
	
	\begin{align*}
		\left| \frac{S_n - nE(f)} n \right| &\le \frac{|S_{k(n)^2} - k(n)^2E(f)| + (n-k(n)^2)(M+E(f))}{k(n)^2} \\
		&\le \left| \frac {S_{k(n)^2} - k(n)^2E(f)}{k(n)^2} \right| + \frac{(k(n)^2 +1) - k(n)^2}{k(n)^2} (M + E(f))
	\end{align*}
	Chacun des termes tend vers $0$, ce qui achève la preuve.
\end{demo}

\begin{demo}[Inégalité de Kolmogorov $\Rightarrow$ théorème 3]
	
	$P(\frac{M_n}{n^\alpha} \ge \eps) \le \frac{\var(f)}{n^{2\alpha -1}\eps^2}$ où $M_n = \underset{1\le k \le n}\max |\tilde S_k|$
	
	On fixe $R\in\N$ tel que $(2\alpha -1)r > 1$.
	
	\[P(\frac{M_{n^r}}{n^{r\alpha}} \ge \eps) \le \frac{\var(f)}{\eps^2 n^{(2\alpha - 1)r}}\]
	
	C'est le terme général d'une série convergente, donc par le premier lemme de Borel-Cantelli, on en déduit que $P(\frac{M_{n^r}}{n^{r\alpha}} \ge \eps \text{ i.o.}) = 0$, et donc que p.p., $\frac{M_{n^r}}{n^{r\alpha}} \rightarrow 0$.
	
	Mais $\frac{M_{n^rr}}{n^{r\alpha}} \rightarrow 0 \Rightarrow \frac{\tilde S_n}{n^\alpha} \rightarrow 0$.
	
	En effet, soit $k(n)$ tel que $(k(n) -1)^r \le n < k(n)^r$. Alors,
	\begin{align*}
		\frac{\tilde S_n}{n^\alpha} &\le \frac{M_{k(n)^r}}{n^\alpha} \\
		& = \frac{M_{k(n)^r}}{k(n)^{\alpha r}} \cdot \frac{k(n)^{\alpha r}}{n^\alpha} \\
		&\le \frac{M_{k(n)^r}}{k(n)^{\alpha r}} \cdot \frac{k(n)^{\alpha r}}{(k(n) -1)^{\alpha r}}
	\end{align*}
	Le premier terme tend vers 0, le second vers 1, ce qui conclut la preuve.
\end{demo}

\begin{demo}[Inégalité de Hoeffding $\Rightarrow$ théorème 3]
	$P(|\tilde S_n| \ge \eps n^\alpha) \le 2\exp \left( \frac{-2\eps^2n^{2\alpha -1}}{C} \right)$
	
	C'est le terme général d'une série convergente, donc d'après le 1er lemme de Borel-Cantelli :
	
	$P(|\tilde S_n| \ge \eps n^\alpha \text{ i.o.})$ ie $\frac{\tilde S_n}{n^\alpha} \rightarrow 0 \text{ p.p.}$.
\end{demo}

\section*{Estimations inférieures}

On suppose que les variables aléatoires prennent au moins deux valeurs avec probabilité non nulle.

\subsection*{Quelques estimations inférieures}

\begin{theorem}
	\[ P((S_n) \text{ bornée} ) = 0\]
\end{theorem}

\begin{theorem}
	\[ P(\limsup \frac{\tilde S_n}{\sqrt n} = +\infty ) = 1 \]
\end{theorem}

\begin{theorem}[Loi du logarithme itéré]
	Presque partout, on a:
	\[ \limsup \frac{\tilde S_n}{\sqrt{2(\var(f))n \ln \ln n}} = 1\]
\end{theorem}

\subsection*{Quelques résultats utiles}

\begin{lemme}[2nd lemme de Borel-Cantelli]
	Si $(A_n)$ est une suite d'événements indépendants, telle que $\underset n \sum P(A_n) = + \infty$, alors $P(\limsup A_n) = 1$.
\end{lemme}

\begin{rem}
	Si $(A_n)$ est une suite d'événements indépendants, alors d'après les deux lemmes de Borel-Cantelli : $P(\limsup A_n) \in \{0,1\}$.
\end{rem}

\begin{demo}[Second lemme de Borel-Cantelli]
	\begin{align*}
		&B_n := \{f_n=a, \dots, f_{n+k}=a\} \\
		&P(B_n) =\underset {\sum = +\infty} {\underbrace{P(f=a)^k}} \\
		\text{Par BC2 ($(B_n)$ indépendants), } &P(B_n \text{ i.o.}) = 1 \\
		\text{Donc } &P(B_n \text{ i.o.}) = 1
	\end{align*}
\end{demo}

\begin{lemme}
	$\forall p, \exists C, \forall n, P_p(S_n=k)\ge \frac 1 {C\sqrt n} \exp{\left( -nC\left| \frac nk n -p\right|^2 \right)}$
\end{lemme}

\begin{demo}
	On cherche en fait à faire une estimation de la loi binomiale $P_p(S_n=k) = \frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}$
	
	$\frac{P_p(S_n=k)}{P_{\frac n k}(S_n=k)} = \frac{p^k(1-p)^{n-k}}{\left(\frac n k\right)^k \left(1- \frac n k\right)^{n-k}} = \exp{\left( -nh\left(\frac k n, p\right)\right)}$ où $h(s,p) = s\ln\left(\frac s p \right) +(1-s)\ln\left(\frac{1-s}{1-p}\right)$.
	
	En particulier, comme $h$ est $\mathcal C^2$ sur un compact, $h(s,p) \le C|s-p|^2$ (à $p$ fixé).
	
	$P_p(S_n=k) \ge P_{\frac k n}(S_n=k)\exp\left( -n \left| \frac k n -p\right|^2\right)$
	
	Il reste à estimer $P_{\frac k n}(S_n=k) = \frac{n!}{k!(n-k)!}\left(\frac k n\right)^k \left( \frac {n-k} n \right)^{n-k}$.
	
	On conclut par Striling.
\end{demo}

\begin{propriete}
	$\forall A > 0, \exists \delta > 0, \forall n\in\N, P(\tilde S_n \ge A\sqrt n) \ge \delta$
\end{propriete}

\begin{demo}
	
	$\begin{aligned}
	P(\tilde S_n \ge A\sqrt n) \overset {\tilde S_n = S_n - np} =& \underset{k\ge np + A\sqrt n} \sum P(S_n=k) \\
	\ge & \underset{np + A\sqrt n \le k \le np +(A+\frac 1 2) \sqrt n} \sum P(S_n=k) \\
	\ge & \sqrt n \frac 1 {C\sqrt n} \exp \left(-nC \frac {(A+\frac 1 2)^2} n \right) \\
	=& \underset{\delta} {\underbrace{\frac 1 C \exp\left( -C(A + \frac 1 2)^2 \right)}}
	\end{aligned}$
\end{demo}

\subsection*{Démonstration des théorèmes 13-15}

\begin{demo}[Théorème 13]
	On se donne $a\ne 0$, $P(a) > 0$.
	 
	\textbf{\quad Affirmation :} Pour tout $k\in\N$, il existe une infinité de $n\in\N$ tels que $f_n = a, \dots, f_{n+k} = a$.
	\begin{demo}
		On démontre cette affirmation en passant au complémentaire.
		
		$B_n := \comp{A_n}$. Les $(B_n)$ sont indépendants.
		
		On veut montrer que $P(\liminf B_n) = 0$ i.e. $P(B_n \text{ APCR}) = 0$.
		
		Or, $\liminf B_n = \underset {n\in\N} \bigcup \  \underset{m\ge n}\bigcap B_m$. Il suffit alors de montrer que $P(\underset{m\ge n}\bigcup B_m) = 0$.
		
		$P(\underset{m=n}{\overset l \bigcap} B_m) \overset{\indep} = \underset{m=n} {\overset l \prod} P(B_m) = \underset{m=n}{\overset l \prod}(1-P(A_m)) \overset{1-x \le e^{-x}} \le e^{-\underset{m=n}{\overset l \sum} P(A_m)} \underset{l\rightarrow +\infty} \longrightarrow 0$.
		
		Donc $P(\underset{m\ge n} \bigcap B_m) = 0$.
	\end{demo}
	
	On en déduit immédiatement le caractère non borné de la suite $(S_n)$
\end{demo}

\begin{demo}[Théorème 14]
	On suppose $A$ "assez grand" (dans un sens qui se précisera dans le corps de la preuve).
	
	On considère la sous-suite $n_k$ telle que $n_1=1$ et $n_{k+1}=n_k+4n_k^2$.
	
	$P\left( \left|\tilde S_{n_{k+1}} - \tilde S_{n_k} \right| \ge a \right) = P\left( \left|\tilde S_{n_{k+1} - n_k} \right| \ge a\right)$.
	
	On considère les événements $B_k = \left\{ \tilde S_{n_{k+1}} - \tilde S_{n_k} \ge A\sqrt{n_{k+1} - n_k} \right\} = \left\{ \tilde S_{4n_k} \ge 2An_k \right\}$.
	
	$B_k$ dépend de $f_{n_k+1} \dots f_{n_{k+1}}$ donc les $(B_k)$ sont indépendants.
	
	Comme $P(B_k) \ge \delta$, d'après BC2, $P(\limsup B_k) = 1$.
	
	Pour un point $\omega \in B_k \text{ i.o.}$, on a donc une infinité de $k$ tels que
	
	$\begin{aligned}
		\tilde S_{n_{k+1}} &\ge \tilde S_{n_k} +2An_k \\
		&\ge n_k(2A+\min f)
		(\text{pour } A \ge |min f|) &\ge An_k
		\end{aligned}$
	
	Or, $n_{k+1} \le 16n_k^2$ donc $n_k \ge \frac {\sqrt{n_{k+1}}} 4$. Donc $\tilde S_{n_{k+1}} \ge \frac A 4 \sqrt{n_{k+1}}$ pour une infinité de $k$. Donc $\limsup \frac {\tilde S_n} {\sqrt n} \ge \frac A 4$. Donc $P(\limsup \frac{\tilde S_n}{\sqrt n} = \infty) = 1$.
\end{demo}

\subsection*{Quelques extensions}

On a montré avec le second lemme de Borel-Cantelli l'existence de séquences qui apparaissent presque partout. Il se peut malgré tout que le temps d'attente soit très long.

\begin{lemme}[Estimation du temps d'attente]
	$a\in \Im f, P(a) > 0$. D'après le second lemme de Borel-Cantelli, la valeur $a$ est prise une infinité de fois.
	
	Soit $T$ le premier temps pour lequel $f_T=a$. Alors $E(T) = \frac 1 {P(a)}$
\end{lemme}

\begin{demo}
	On montre facilement que $T$ suit une loi géométrique, et en utilisant $E(T) = \underset{k\ge 1} \sum P(T\ge k)$, on conclut rapidement.
\end{demo}

\end{document}