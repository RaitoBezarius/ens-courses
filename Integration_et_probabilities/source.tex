% !TeX spellcheck = fr-FR


\documentclass[10pt,a4paper,notitlepage ]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{wasysym}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{tcolorbox}
\usepackage{stmaryrd}
\usepackage[left=2.00cm, right=2.00cm, top=1.50cm, bottom=1.50cm]{geometry}

\everymath{\displaystyle}

\title{Intégration et probabilités}
\date{Premier semestre 2020}
\author{Paul Fournier et Constantin Vaillant-Tenzer, d'après le cours de Patrick Bernard}

\newcommand{\N}{\mathbb N}
\newcommand{\R}{\mathbb R}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Z}{\mathbb Z}
\newcommand{\T}{\mathcal T}
\newcommand{\A}{\mathcal A}
\newcommand{\B}{\mathcal B}
\newcommand{\dd}{\ \mathrm d}
\newcommand{\1}{\mathds 1}
\newcommand{\LL}{\mathcal L}
\newcommand{\eps}{\varepsilon}
\newcommand{\comp}[1]{#1^\complement}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\indep}{\perp \!\!\! \perp}
\renewcommand{\Im}{\mathrm{Im }}
\newcommand{\borel}{\mathrm{Borel}}
\newcommand{\prob}{\mathbb P}

\newenvironment{definition}[1][]{
	
	\textbf{Définition #1 : }
}
{}

\newcounter{th}
\newenvironment{theorem}[1][]{
\refstepcounter{th}
\begin{tcolorbox}
	\textbf{Théorème \theth \ #1}
	
	
}{\end{tcolorbox}}

\newenvironment{propriete}[1][]{
	\begin{tcolorbox}
		\textbf{Propriété #1 : }
}
{\end{tcolorbox}}

\newenvironment{demo}[1][]{

	\textbf{Démonstration #1 :}
}{\begin{flushright}
	$\square$
\end{flushright}
}

\newenvironment{exo}{
	
	\textbf{Exercice :} }{}

\newenvironment{exemple}{
	
	\textbf{Exemple :} }{}

\newenvironment{corollaire}{
	\begin{tcolorbox}
		\textbf{Corollaire : }
	}
	{\end{tcolorbox}}

\newenvironment{lemme}[1][]{
	\begin{tcolorbox}
		\textbf{Lemme #1 : }
	}
	{\end{tcolorbox}}

\newenvironment{rem}{
	
		\textbf{Remarque :}}{}

\begin{document}
	\maketitle
	
	\tableofcontents
	
\chapter{Préliminaires}

\section{Introduction}
	Références : 
	\begin{itemize}
		\item \textsc{Billingsley}, \textit{Probabiliy and measure}
		\item \textsc{Kolmogorov \& Fomin}, tome 2 
	\end{itemize}
	Motivations :
	\begin{itemize}
		\item Définir la longueur d'une partie de $\mathbb R$
		\item Définir l'aire d'une partie de $\mathbb R ^2$
		\item  Définir $\int f \mathrm dx$ pour $f : \mathbb R ^d \rightarrow \mathbb R$
		\item Définir, préciser la notion mathématique décrivant une suite infinie de jets de dés
	\end{itemize}
Par exemple :
\begin{itemize}
	\item Si $f : \mathbb R \rightarrow \mathbb R$, on peut définir $\int f$ comme l'aire algébrique définie par le graphe de $f$. Ainsi, définir une aire permet de définir une intégrale
	\item De même, $\lambda(A) = \mathds 1_A$ avec $\mathds 1_A(x)=1$ ssi $x \in A$. Donc définir une intégrale revient à définir une mesure.
	\item Tirer un nombre au hasard dans $[\,0,1]\,$, cela revient à tirer au hasard la suite de ses décimales au D10, car on mesure une partie de $\{ 0, 1, \dots 9 \}^\mathbb N$
 \end{itemize}

On se demande alors comment définir la surface d'une partie du plan.

\emph{Méthode 1 :} à la Riemann. On approxime avec un quadrillage. On compte le nombre de carrés qui intersectent l'ensemble considéré, puis on conclut en passant à la limite quand le côté du quadrillage tend vers $0$.

\emph{Méthode 2 :} on pose $\lambda(A) := \underset{(R_i)}{\inf} \sum_{i=1}^{\infty}  \lambda(R_i)$ où $R_i$ est une suite de rectangles recouvrant $A$. \\

À noter : les deux méthodes ont des cas pathologiques différents.

\section{Ensembles dénombrables}

\begin{definition}
	Un ensemble est dénombrable ssi il est en bijection avec $\mathbb N$
\end{definition}
\begin{propriete}
	Toute partie d'un ensemble dénombrable est au plus dénombrable
\end{propriete}
\begin{demo}
	On pose $x : \mathbb N \rightarrow X, Y \subset X$. Si $Y$ n'est pas fini :
	\begin{align*}
	&i_1 = \min \{i\in\mathbb N, x_i \in Y \} \\
	&\dots  \\
 	&i_n = \min \{ i\in \mathbb N, x_i\in Y \setminus\{x_1, \dots,x_{n-1}\}\}
 	\end{align*} 
	Ainsi, $k \mapsto x_{n_k}$ est une bijection de $\mathbb N$ vers $Y$.
\end{demo}

\begin{propriete}
	L'image d'une suite est au plus dénombrable.
\end{propriete}
\begin{demo}
	On note $x:\mathbb N \rightarrow X$ une suite.
On crée de manière analogue une sous-suite injective de $x$ de même image que $x$ (sauf si $f(x(\mathbb N))$ est fini).
\end{demo}
\begin{propriete}
	$\mathbb N \times \mathbb N$ est dénombrable.
\end{propriete}
\begin{demo}
	$(n_1,n_2) \mapsto 2^{n_1} (2n_2+1) -1$ est une bijection $\mathbb N^2 \rightarrow \mathbb N$.
\end{demo}

\begin{propriete}
	Une réunion au plus dénombrables d'ensembles au plus dénombrable est au plus dénombrable.
\end{propriete}

\begin{demo}
	On traite le cas "union dénombrable d'ensembles dénombrables".
	
	Soit $A_i$ des parties dénombrables d'un ensemble $X$.
	Pour tout $i$, il existe $b_i : \mathbb N \rightarrow A_i$ bijection. (nb : ceci requiert en fait l'axiome du choix dénombrable)
	Alors$ \begin{aligned}
		(i,j) &\mapsto b_i(j) \\
		\mathbb N ^2 & \rightarrow \underset{i}{\bigcup} A_i
	\end{aligned}$ est surjective.

Donc $\underset{i}{\bigcup}A_i$ est au plus dénombrable.

Or $\underset{i}{\bigcup}A_i \supset A_i$.

Donc $\underset{i}{\bigcup} A_i$ est dénombrable.
\end{demo}

\begin{propriete}
	Si $X$ est dénombrable, $\mathcal P(X)$ ne l'est pas.
	
	Plus généralement, quel que soit $X$, $X$ et $\mathcal P(X)$ ne sont jamais en bijection (théorème de Cantor).
\end{propriete}

\begin{demo}
	Supposons qu'il existe $x: \begin{aligned} X & \rightarrow \mathcal P(X) \\
		x & \mapsto A_x \end{aligned}$ une bijection.
	
	Considérons $B := \{x, x\notin A_x\}$. Comme $x$ est une bijection, il existe $y \in X$ tel que $B = A_y$.
	
	Question : a-t-on $y\in B$. On arrive à un paradoxe type Russel.
\end{demo}

\begin{exo}
\begin{itemize}	
	\item $\{0,1\}^\mathbb N$ est non dénombrable.
	
	\item $\mathbb R$ est non dénombrable.
	
\end{itemize}
\end{exo}

\section{$\limsup$ et $\liminf$}

\begin{definition}
	
Soit $(x_n)_{n\in\mathbb N} \in \mathbb R ^\mathbb N$ (plus généralement $\in\bar{\mathbb R}^\mathbb N$). Alors $s_n := \underset{k\geq n}{\sup} x_k$.

$s_n$ est décroissante (donc a une limite dans $\bar{\mathbb R})$.

Alors $\lim s_n =: \limsup x_n = \inf s_n$.

De même pour $\liminf x_n$. 
\end{definition}
\begin{propriete}
	$\lim x_n$ existe ssi $\liminf x_n = \limsup x_n$. Dans ce cas, $\lim x_n = \limsup x_n = \liminf x_n$.
\end{propriete}

\begin{demo}
	$\Leftarrow$ : $i_n \leq x_n \leq s_n$. On conclut par théorème d'encadrement.
	
	$\Rightarrow$ : Si $x_n \rightarrow l$ alors : $\forall \varepsilon > 0, \exists N \in \mathbb N, \forall n \geq N, l-\varepsilon \leq i_n \leq l \leq s_n \leq l+\varepsilon$.
	Donc $s_n \rightarrow l$ et $i_n \rightarrow l$.
\end{demo}

\begin{propriete}
	Si $y_n$ est une sous-suite de $x_n$, alors $\liminf x_n \le \liminf y_n \le \limsup y_n \le \limsup x_n$
\end{propriete}

Ainsi, si $l$ est valeur d'adhérence de $x_n$, alors $\liminf x_n \le l \le \limsup x_n$.
\begin{propriete}
	$\limsup x_n = -\liminf (-x_n)$
\end{propriete}

\begin{propriete}
	Il existe une sous-suite de $x_n$ qui converge vers $\limsup x_n$. Idem pour $\liminf x_n$.
\end{propriete}
\begin{demo}
	On choisit $k_n \ge n$ tel que $s_n - \frac{1}{n} \le x_{k_n} \le s_n$. $n \mapsto x_{k_n}$ converge vers $\limsup x_n$.
\end{demo}

\section{Familles sommables}

On pose $(a_i)_{i\in I}$ famille de nombres positifs.

\begin{definition}
	$\sum_{i \in I} a_i := \underset{F \subset I \mathrm{fini}}{\sup}\sum_{i \in F}a_i$
\end{definition}
\begin{propriete}
	Si $\sum_{i \in I} a_i$ est fini, alors $\{i \in I, a_i\neq 0\}$ est au plus dénombrable.
\end{propriete}

\begin{demo}
	$\{i\in I, a_i \in \mathbb R \setminus \{0\}\} \subset \underset{k \in \mathbb N}{\bigcup}
	\underset{\# \le k\sum_{i\in I} a_i}{\underbrace{\{i \in I, a_i \ge \frac{1}{k}\}}}$
\end{demo}

À partir de maintenant, on considérera $I$ dénombrable.

\begin{propriete}
	Si $\sigma : \mathbb N \rightarrow I$ est une bijection, alors
	$\sum_{i\in I}a_i = \underset{n \rightarrow +\infty}{\lim}\sum_{k=1}^{n}a_{\sigma (k)} =: \sum_{k=1}^{+\infty}a_{\sigma(k)}$
\end{propriete}

\begin{demo}
	$\forall F \subset I$ fini, $\sigma ^{-1}(F)$ est fini donc majoré par un entier $N$.
	
	$\sum_{i\in F}a_i = \sum_{k\in \sigma^{-1}(F)}a_{\sigma(k)} \le \sum_{k=1}^N a_{\sigma(k)} \le \sum_{k=1}^{+\infty}a_{\sigma(k)}$
	
	Donc par passage au sup : $\sum_{i\in I} a_i \le \sum_{k=1}^{+\infty}a_{\sigma(k)}$.
	
	Réciproquement, $\sum_{k=1}^N a_{\sigma(k)} = \sum_{i\in\sigma(\llbracket 1,N \rrbracket)} a_i \le \sum_{i\in I}a_i$. On conclut par passage à la limite.
\end{demo}

\begin{corollaire}
	Si $(a_k) \in \mathbb R_+^{\mathbb N}, \sum_{k=1}^{+\infty}a_k=\sum_{k=1}^{+\infty}a_{\sigma(k)}$ et ce quel que soit $\sigma : \mathbb N \rightarrow \mathbb N$ bijection.
\end{corollaire}

En particulier dans le cas $I=\mathbb N^2, (a_{i,j})_{(i,j)\in I}\in \mathbb R_+^I$ :
\begin{propriete}
	$\sum_{(i,j)\in I}a_{i,j} = \sum_{i=1}^{+\infty}\left(\sum_{j=1}^{+\infty}a_{i,j}\right) =
	\sum_{j=1}^{+\infty}\left(\sum_{i=1}^{+\infty}a_{i,j}\right)$
\end{propriete}

\begin{demo}
	$F\subset I$ fini. Il existe $N \in \mathbb N$ tel que $F \subset \llbracket 1,N\rrbracket^2$. Donc $\sum_{(i,j)\in F}a_{i,j} \le \sum_{i=1}^{N}\sum_{j=1}^{N}a_{i,j} \le
	\sum_{i=1}^N\sum_{j=1}^{+\infty}a_{i,j} \le
	\sum_{i=1}^{+\infty}\sum_{j=1}^{+\infty}a_{i,j}$.
	
	Réciproquement, $\forall N\in\mathbb N, \forall M \in\mathbb N, \sum_{i=1}^N\sum_{j=1}^Ma_{i,j} \le
	\sum_{(i,j)\in \mathbb N^2}a_{i,j}$.
	
	Donc $(M\rightarrow +\infty)$, $\sum_{i=1}^N\sum_{j=1}^{+\infty}a_{i,j} \le
	\sum_{(i,j)\in \mathbb N^2}a_{i,j}$.
	
	Donc $(N\rightarrow +\infty)$, $\sum_{i=1}^{+\infty}\sum_{j=1}^{+\infty}a_{i,j} \le
	\sum_{(i,j)\in \mathbb N^2}a_{i,j}$.
\end{demo}

\section{Séries absolument convergentes}

Soit $(a_i)_{i\in I}$ une famille de réels tels que $\sum_{i\in I}|a_i|$ soit finie.

On définit $a_i^+ := \max(a_i,0)$, $a_i^- := \max(-a_i, 0)$.

Donc $a_i^+ - a_i^- = a_i$ et $a_i^+ + a_i^- = |a_i|$.

\begin{propriete}
	$\sum_{i\in I}a_i^+ - \sum_{i\in I}a_i^- = \sum_{k=1}^{+\infty}a_{\sigma(k)}$ et ce quel que soit $\sigma : \mathbb N \rightarrow I$ bijection.
\end{propriete}

\begin{demo}
	$\sum_{i\in I}a_i^+ \le \sum_{i\in I} |a_i|$ donc la somme est finie. Idem pour $\sum_{i\in I}a_i^-$.
	
	$\sum_{k=1}^n a_{\sigma(k)} = \sum_{k=1}^n a_{\sigma(k)}^+ - \sum_{k=1}^n a_{\sigma(k)}^- \underset{n\rightarrow +\infty}{\rightarrow}
	\sum_{k=1}^{+\infty}a_{\sigma(k)}^+ - \sum_{k=1}^{+\infty}a_{\sigma(k)}^-$
\end{demo}
\begin{corollaire}
	Sous réserve de convergence absolue, on a :
	
	\[\sum_{k=1}^{+\infty}a_k = \sum_{k=1}^{+\infty} a_{\sigma(k)}\]
	
	\[\sum_{i=1}^{+\infty}\sum_{j=1}^{+\infty}a_{i,j} =
	\sum_{j=1}^{+\infty}\sum_{i=1}^{+\infty}a_{i,j}\]
\end{corollaire}

\chapter{Probabilités}
\section{Vocabulaire}

\begin{definition}
	Soit $X$ un ensemble. On dit que $\mathcal A \subset \mathcal P(X)$ est :
	\begin{itemize}
		\item une algèbre (d'ensembles) si elle est stable par union finie, intersection finie et passage au complémentaire, contient $\emptyset$ et $X$.
		\item une tribu (ou $\sigma$-algèbre) si c'est une algèbre stable par réunion/intersection dénombrable.
	\end{itemize}
\end{definition}

\begin{exemple}
	\begin{itemize}
		\item $\mathcal P(X)$ est une tribu.
		\item $\{\emptyset, X\}$ est une tribu.
	\end{itemize}
\end{exemple}

Si on se donne une partition finie de $X$ : $X=X_1 \sqcup X_2 \dots \sqcup X_k$, alors l'ensemble des $A \subset X$ de la forme $A=\underset{n\in I \subset \llbracket 1,k \rrbracket}{\bigcup}X_n$ est une tribu finie.

\begin{lemme}
	Toute algèbre finie est associée à une partition finie.
\end{lemme}

\begin{demo}
	Soit $\mathcal A$ une algèbre finie.
	
	$\forall x \in X, A(x) := \underset{x\in A}{\underset{A\in \mathcal A}{\bigcap}} A$.
	
	Pour $x$ et $y$ donnés, soit $A(x) = A(y)$, soit $A(x) \cap A(y) = \emptyset$.
	
	Fixons $x\in X, B\in\mathcal A$.
	\begin{itemize}
		\item Soit $x\in B$ et alors $A(x)\subset B$.
		\item Soit $x\in \comp B$ et alors $A(x)\subset \comp B$ i.e. $A(x)\cap B = \emptyset$
	\end{itemize}
	On conclut avec $B=A(y)$.
\end{demo}

\begin{definition}
	Si $\mathcal A$ est une algèbre de $X$ et $m:\mathcal A \rightarrow [0,+\infty]$ une fonction.
	
	On dit que $m$ est une \emph{mesure additive} si :
	\begin{itemize}
		\item $m(\emptyset) = 0$
		\item $m(A\sqcup B) = m(A) + m(B) \qquad (A\cap B = \emptyset)$
	\end{itemize}
\end{definition}

\begin{definition}
	Si $\mathcal T \subset \mathcal P(X)$ est une tribu, $m:\mathcal T \rightarrow [0,+\infty]$ est une \emph{mesure} si :
	\begin{itemize}
		\item $m(\emptyset) = 0$
		\item $m(\underset{i\in I}{\bigsqcup}A_i) = \sum_{i\in I}m(A_i)$ pour $(A_i)_{i\in I}$ famille dénombrable disjointe.
	\end{itemize}
\end{definition}
\begin{rem}
	Toute mesure est une mesure additive.
\end{rem}

\begin{rem}
On appelle parfois les mesures "mesures $\sigma$-additives".
\end{rem}

\begin{rem}
	Lorsque $m:\mathcal A \rightarrow [0,+\infty]$ est une mesure additive sur une algèbre, les propriétés suivantes sont équivalentes :
	\begin{enumerate}
		\item Si $A_i\in\mathcal A$ sont disjoints, $(A_i)$ dénombrable, $\underset{i\in I}{\bigsqcup}A_i \in \mathcal A$, alors $m(\underset{i\in I}{\bigsqcup}A_i) = \sum_{i\in I}m(A_i)$
		\item Si $A,A_i\in\mathcal A$, $A\subset \underset{i\in I}{\bigcup}A_i$, alors $m(A) \le \sum_{i\in I}m(A_i)$.
	\end{enumerate}
Dans ce cas, on dit que $m$ est $\sigma$-additive.
\end{rem}

\begin{demo}
	$(1) \Rightarrow (2)$ :
	
	Soit $A_i \in \mathcal A$. On définit $\tilde{A_i}$ par : $\tilde A_1 = A_1, \dots \tilde A_n = A_n\setminus \tilde A_{n-1} \quad \forall n \ge 1$
	
	Alors $\bigcup A_i = \bigsqcup \tilde A_i$.
	
	Si $A \subset \bigcup A_i$, alors $A \subset \bigsqcup \tilde A_i$. Alors $A=\bigsqcup(A\cap\tilde A_i)$.
	
	Donc $m(A)=m(\bigsqcup (\tilde A_i \cap A)) \le \sum m(A_i)$.

	$(2) \Rightarrow (1)$ :
	
	Si $A=\bigsqcup A_i \overset{(2)}{\Rightarrow} m(A) \le \sum m(A_i)$.
	
	$A\supset\overunderset{n}{i=1}{\bigsqcup}A_i$ quel que soit $n$.
	
	Donc $m(A) \ge \sum_{i=1}^n m(A_i)$. Donc ($n \rightarrow +\infty$), $m(A) \ge \sum_{i=1}^{+\infty}m(A_i)$.
	
\end{demo}

\begin{definition}
	Soit $f:\Omega \rightarrow X$ une application. Si $\mathcal A$ est une algèbre (ou une tribu) sur $\Omega$, alors on définit l'algèbre (tribu) image par :
	
\[ f_*\mathcal A = \{A\subset X, f^{-1}(A)\in \mathcal A\}\]
	Si $\mathcal A$ est une algèbre (tribu) sur $X$, alors \[f^*\mathcal A = \{f^{-1}(A), A\in\mathcal A\}\] est une algèbre (tribu) sur $\Omega$.
\end{definition}

La vérification du fait que $f^*\mathcal A$ et $f_*\mathcal A$ est une algèbre (tribu) découle des propriétés des préimages :

\begin{align*}
	&f^{-1}(A\cap B) = f^{-1}(A)\cap f^{-1}(B) \\
	&f^{-1}(A \cup B) = f^{-1}(A) \cup f^{-1}(B) \\
	&f^{-1}(A \setminus B) = f^{-1}(A) \setminus f^{-1}(B) 
\end{align*}

\begin{definition}
	Si $f:(\Omega, \mathcal A, m) \rightarrow X$ est une application, on définit la mesure image (ou la loi) comme la mesure :
	\[ (f_*m)(Y) := m(f^{-1}(Y))\] définie sur $f_*\mathcal A$.
\end{definition}

\begin{definition}
	$m$ est dite finie ssi $m(X) < +\infty$
\end{definition}
\begin{definition}
	$m$ est dite de probabilité ssi $m(X)=1$
\end{definition}

\begin{definition}
	$f:(\Omega,\tau) \rightarrow (X,\mathcal T)$ est dite mesurable si : \[\forall Y \in \mathcal T, f^{-1}(Y) \in \tau\] i.e.
	\begin{align*}
		&f_*\tau \supset \mathcal T \\
		&f^*\mathcal T \subset \tau
	\end{align*}
\end{definition}

\begin{exo}
	Soit $\Omega, X$ des ensembles, $\mathcal T$ une tribu sur $X$. Soit $f:\Omega \rightarrow X$ une application, $g:\Omega\rightarrow X$ une application à valeurs dans un ensemble fini $Y$. Alors $g$ est $f^*\mathcal T$ mesurable ssi $\exists h:(X,\mathcal T) \rightarrow (Y,\mathcal P(Y))$ mesurable telle que $g=h \circ f$. i.e. "$g$ est $f$-mesurable ssi $g$ ne dépend que de $f$".
\end{exo}

\section{Modélisation d'une expérience aléatoire finie (ex : jets de dés)}

Soit $Y$ un ensemble fini représentant les issues possibles. Il y a 2 manières de représenter un tirage aléatoire sur $Y$.
\begin{enumerate}
	\item On se donne une mesure de probabilité sur $(Y,\mathcal P(Y))$. Pour ceci, il suffit de donner $p:Y\rightarrow[0,1]$ tel que $\sum_{y\in Y}p(y) = 1$. On note $P$ la mesure de probabilité ainsi créée.
	\item On se donne un espace de probabilité abstrait $(\Omega,\mathcal T, \mathbb P)$ et une application mesurable $f:\Omega \rightarrow Y$ telle que $f_*\mathbb P=P$.
\end{enumerate}
Pour passer de 1. à 2., il suffit de prendre $\Omega = Y$, $\mathcal T = \mathcal P(Y)$, $\mathbb P = P$, $f=\mathrm {id}$.

L'expérience aléatoire consistant à jeter un nombre fini $k$ de dés de valeurs possibles $Y_1, \dots, Y_k$ est simplement une expérience aléatoire à valeurs dans le produit $Y=Y_1 \times Y_2 \times \dots\times Y_k$.

La description en termes de variables aléatoires consiste donc à se donner une application mesurable $f : (\Omega, \mathcal T, P) \rightarrow Y$, c'est à dire $k$ applications mesurables $f_i : (\Omega, \mathcal T, P) \rightarrow Y_i$, définies sur \emph{le même espace de probabilités}.

\begin{definition}
	La loi de $f$ (qui est une probabilité sur $Y$) est dite \emph{loi jointe}. Les lois des $f_i$ (qui sont des probabilités sur $Y_i$) sont dites \emph{lois marginales}.
\end{definition}

\begin{rem}
	La loi jointe détermine les lois marginales, qui peuvent se décrire explicitement par $m_i(y_i)=\underset{y_1,\dots,y_{i-1},y_{i+1},\dots,y_k}{\sum} m(y_1,\dots,y_k)$.
	
	Plus abstraitement, ce soint les mesures images $m_i=(\Pi_i)_*m_i$ où $\Pi_i : Y \rightarrow Y_i$ est la projection.
\end{rem}

\begin{rem}
	La loi jointe est déterminée par $|Y_1|\times\dots\times |Y_k| - 1$ nombres réels ($-1$ à cause de la contrainte $\sum p = 1$).
	
	Les lois maginales sont déterminées par $|Y_1| + \dots + |Y_k| - k$ nombres réels, ce qui est beaucoup moins.
\end{rem}

Si on se donnes les marginales $m_1, \dots, m_k$, ilm existe de nombreuses lois jointes qui engendrent ces marginales. L'une d'entre elles est particulièrement intéressante : la loi produit $m((y1_,\dots,y_k))=m_1(y_1) \cdot \dots \cdot m_k(y_k)$, qui correspond (par définition) à des expériences indépendantes.

\begin{definition}
	\begin{itemize}
		\item Les événement $A,B$ dans un espace de probabilité $(\Omega,\mathcal T, P)$ sont dits indépendants si $P(A\cap B) = P(A)P(B)$.
		\item Si $(X_i, \mathcal T_i)_{1\le i\le k}$ sont des espaces mesurables (c'est à dire munis de tribus $\mathcal T_i$), les variables aléatoires (applications mesurables) $f_i : (\Omega,\mathcal T, P) \rightarrow (X_i,\mathcal T_i)$ sont dites \emph{indépendantes} si $\forall Z_i \in \mathcal T_i, P(f_1\in Z_1, \dots, P_k \in Z_k) = P(f_1 \in Z_i) \cdot \dots \cdot P(f_k \in Z_k)$
	\end{itemize}
\end{definition}

\begin{propriete}
	Les événements $A$ et $B$ sont indépendants ssi les variables aléatoires $\mathds{1}_A, \mathds{1}_B : (\Omega, \mathcal T, P) \rightarrow \{0,1\}$ le sont.
\end{propriete}

\begin{demo}
	Il suffit de montrer que $\comp A$ et $B$ sont indépendants (le reste est évident ou vient par symétrie).
	\begin{align*}
		P(\comp A\cap B) &= P((\Omega \setminus A) \cap B) \\
		&= P(B \setminus A\cap B) \\
		&= P(B) - P(A\cap B) \\
		&= P(B) - P(A)P(B) \\
		&= (1-P(A))P(B) \\
		&= P(\comp A)P(B)
	\end{align*} 
\end{demo}

\begin{definition}
	Les événements $A_1,\dots,A_k$ sont dits indépendants si $\mathds 1_{A_1} \dots \mathds 1_{A_k} : \Omega \rightarrow \{0,1\}$ le sont.
\end{definition}

\begin{rem}
	Il ne suffit pas d'avoir l'indépendance deux à deux ou $P(A_1 \cap \dots \cap A_k) = P(A_1) \cdot \dots \cdot P(A_k)$.
\end{rem}

\begin{propriete}
	Il suffit d'avoir $P(A_{i_1} \cap \dots \cap A_{i_k}) = P(A_{i_1}) \cdot \dots \cdot P(A_{i_k})$, et ce $\forall \{i_1, \dots, i_k\} \subset \llbracket 1,k \rrbracket$.
\end{propriete}

\begin{demo}
	Il faut montrer que
	\[ (*) \quad P(B_1 \cap \dots \cap B_k) = P(B_1) \cdot \dots \cdot P(B_k) \forall B_i \in \{\emptyset, A_i, \comp{A_i}, \Omega\}	\]
	Il découle de l'hypothèse que c'est vrai pour $B_i \in \{ \emptyset, A_i, \Omega\}$.
	
	Il suffit donc de constater que $(*)$ implique $P (\comp{B_1} \cap B_2 \cap \dots \cap B_k) = P(\comp{B_1})P(B_2) \cdot \dots \cdot P(B_k)$, ce qui se montre comme ce-dessus. On conclut par récurrence finie.
\end{demo}

\begin{exemple}
	Tirage non indépendant :
	
	On tire $-$ chiffres dans $\llbracket 1,6 \rrbracket$, en leur imposant d'être distincts. La loi jointe est donc : $P(y_1, \dots, y_6) = \begin{cases}
		0 &\mathrm{\ si\  non\  distincts} \\
		\frac{1}{6!} &\mathrm{\ si\ distincts}
	\end{cases}$.

	Les lois marginales sont : $P_1(y_1):= \underset{y_2,\dots,y_k}{\sum} P(y_1,\dots,y_k) = \frac{5!}{6!} = \frac{1}{6}$. Les lois marginales sont donc les mêmes que pour un tirage indépendant !
\end{exemple}
\begin{definition}
	On dit que $f_i,\ i\in I$ sont indépendantes si $f_i,\ i\in F$ le sont pour tout $F\subset I$ fini.
\end{definition}

\section{Modélisation d'une suite infinie de jets dés indépendants}

Donnons-nous une suite infinie d'espaces de probabilités finis $(Y_i,P_i)$ (la tribu est $\mathcal P(Y_i)$).

Pour chaque $n$, on a vu que l'on peut trouver des variables aléatoires indépendantes $f_i:(\Omega_n,\mathcal T_n, P_n) \rightarrow Y_i$ de loi $P_i$.

Question : peut-on prendre $(\Omega_n,\mathcal T_n, P_n)$ indépendant de $n$?

\begin{theorem}
	Il existe un espace de probabilité $(\Omega,\mathcal T, P)$ et une suite de variables aléatoires $f_i : \Omega \rightarrow Y_i$ qui sont indépendantes et de loi $P_i$.
\end{theorem}

\begin{rem}
	Les variables aléatoires $f_i$, $i\in \mathbb N$ sont indépendantes ssi $f_1, \dots, f_n$ le sont pour tout $n$.
\end{rem}

L'hypothèse d'indépendance consiste donc à dire que, pour tout $n$ et pour tout $(y_1, \dots, y_n) \in Y_1 \times \dots \times Y_n$, l'événement $\{f_1=y_1,\dots,f_n=y_n\}$ est mesurable ()$\in\mathcal T$) et de mesure $P(f_1=y_1, \dots f_n=y_n) = P_1(y_1)\cdot\dots\cdot P_n(y_n)$.

En termes de loi, ceci implique que $\{y_1\}\times \dots \{y_n\} \times Y_{n+1} \times \dots$ est mesurable sur $X:=\prod Y_i$ et que sa mesure est $m(\{y_1\}\times \dots \times Y_{n+1}) = P_1(y_1) \cdot \dots \cdot P_n(y_n)$.

\section{Introduction de l'algèbre $\mathcal A_\infty$ engendrée par les cylindres finis}

Sur le produit $X = \prod Y_i$, pour $n$ fixé, les ensembles de la forme $\{y_1\}\times \dots \times \{y_n\}\times Y_{n+1} \times \dots$ forment une partition finie (ce sont les cylindres finis), qui engendre une algèbre finie $\mathcal A_n$ (qui est donc aussi une tribu).

C'est l'algèbre engendrée par les $n$ premières coordonnées. En effet si $\Pi : X \rightarrow Y_1 \times \dots \times Y_n$ est la projection, alors $\mathcal A_n = \Pi^* (\mathcal P(Y_1 \times \dots \times Y_n))$.

Cette algèbre décrit les parties de $X$ qui peuvent être décrites en termes des $n$ premières coordonnées.

On a $\mathcal A_n \subset \mathcal A_{n+1}$. On note $\mathcal A_\infty = \underset{n\ge 1}{\bigcup} \mathcal A_n$.

$\mathcal A_\infty$ est donc l'algèbre des parties de $X$ qui dépendent d'un nombre fini de coordonnées. C'est l'algèbre engendrée par les cylindres finis.

Contrairement aux $\mathcal A_n$, $\mathcal A_\infty$ est infinie et ce n'est pas une tribu !

L'hypothèse d'indépendance des $f_i$ implique que la loi $m$ doit être définie sur $\mathcal A_\infty$, et qu'elle y est déterminée par la relation \[ (*) \quad m(\{y_1\}\times \dots \times \{y_n\} \times Y_{n+1} \times \dots ) = P_1(y_1) \cdot\dots\cdot P(y_n)\]

\begin{theorem}
	Il existe sur $X = \prod Y_i$ une tribu $\tau$, qui contient $\mathcal A_\infty$, et une mesure $m$ sur $\mathcal T$ qui vérifie $(*)$.
\end{theorem}

On vient en fait de voir que le théorème 1. implique le théorème 2.
Réciproquement, il suffit de prendre $\Omega = X, \mathcal T = \tau, P=m, f=\text{projection}$.

Pour démontrer l'utilité du théorème 2., donnons des exemples d'ensembles qu'il est naturel de considérer et qui sont dans $\tau$ mais pas dans $\mathcal A_\infty$. On suppose $Y_i \subset \mathbb R$

\begin{exemple}
	L'ensemble $\{(y_i)\in X, \frac{y_1 + \dots y_n}{n} \rightarrow l\}$ est mesurable. En effet, il s'écrit : $\underset{k\ge 1}{\bigcap}\ \underset{n\in \mathbb N}{\bigcup}\ \underset{m \ge n}{\bigcap} \{\left| \frac{y_1+ \dots y_n}{n} - l \right| \le \frac{1}{k}\}, \text{ i.e. } \forall k \ge 1, \exists n\in \mathbb N, \forall m\ge n, \dots$. 
	Chacun des ensembles est dans $\mathcal A_\infty$ donc l'ensemble considéré est dans $\tau$.
\end{exemple}

\section{Quelques résultats d'extension des mesures}
	Si $A_n$ est une suite d'ensembles, on note :
	\[ \liminf A_n = \underset{n}{\bigcup}\ \underset{m\ge n}{\bigcap} A_m = \{A_i \text{ APCR}\}\]
	\[ \limsup A_n = \underset{n}{\bigcap}\ \underset{m\ge n}{\bigcup} A_m = \{A_i \text{ infinitely often (i.o.)}\}\]
	
	Si $\tau$ est une tribu, que les $A_n \in \tau$, alors $\limsup A_n \in \tau$ et $\liminf A_n \in \tau$.
	
	\begin{propriete}
		\begin{itemize}
			\item $\liminf A_n \subset \limsup A_n$
			\item $\liminf \comp{A_n}= \comp{(\limsup A_n)}$
		\end{itemize}
	\end{propriete}
	\begin{demo}
		$\forall m,M, \quad \underset{n\ge m}{\bigcap} A_n \subset \underset{n \ge M} A_n$.
		Donc $\underset{n\ge m}{\bigcap} A_n \subset \limsup A_n$, donc $\liminf A_n \subset limsup A_n$
	\end{demo}
\begin{exo}
	$\limsup \mathds 1_{A_n} = \mathds 1_{\limsup A_n}$
\end{exo}

\begin{exemple}
	On considère un tirage aléatoire indépendant $f_n \in {-1, 1}^{\mathbb N}$, ce que l'on voit comme un jeu de hasard (le joueur gagne ou perd $1$ à chaque étape). Étant donnée la richesse initiale $r_0$ et un objectif $R$, on considère l'événement \{le joueur atteint la richesse $R$ avant de se ruiner\}.
	
	Il s'écrit $\underset{n \ge 1}{\bigcup} \{y_1 + \dots y_n \ge -r_0 \quad \forall k < n \text{ et } y_1 + \dots + y_k = R-r_0\}$.
	
	C'est une réunion dénombrable d'éléments de $\mathcal A_\infty$
\end{exemple}

Le théorème 2 sera déductible du théorème suivant :
\begin{theorem}[Hahn-Kolmogorov]
	Soit $\mathcal A$ une algèbre d'ensembles sur $X$. Soit $\underline m$ une mesure de probabilité additive sur $\mathcal A$, qui vérifie la propriété de $\sigma$-additivité.
	
	Alors il existe une tribu $\tau$ contenant $\mathcal A$, et une mesure de proba $m$ sur $\tau$ qui prolonge $\underline m$. De plus, on peut prendre : $m(B) = \underset{B \subset \bigcup A_i} \inf \  \underset{i \in \mathbb N}{\sum} \underline m (A_i)$, où le $\inf$ est pris sur les recouvrements dénombrables de $B$ par des éléments de $\mathcal A$.
\end{theorem}

Pour démontrer le théorème 2, on va appliquer le théorème 3 avec $\mathcal A = \mathcal A_\infty$, et $\underline m$ la mesure additive déterminée par $\underline m (\{y_1\}\times \dots \times \{y_n\} \times Y_{n+1} \times \dots) = P_1(y_1) \dots P_n(y_n)$.

Il nous suffit donc de vérifier que cette mesure additive a la propriété de $\sigma$-additivité.

\begin{propriete}
	Toute mesure additive sur $\mathcal A_\infty$ est $\sigma$-additive.
\end{propriete}

\begin{demo}
	Soient $A \in \mathcal A_\infty$ et $A_i \in \mathcal A_infty$ tel que $A \subset \underset i \bigcup A_i$, alors $\exists n, A \subset \bigcup_{i=1}^n A_i$.
	
	\begin{itemize}
		\item méthode savante : c'est la compacité de $A$ dans $X$ muni de la topologie produit (les $A_i$ sont ouverts et compacts)
		\item à la main :
		On pose $B_n = A \setminus \bigcup_{i=1}^n A_n$. On veut montrer que $\exists n, B_n = \emptyset$, sachant que $\underset {n\ge 0}\bigcap B_n = \emptyset$.
		
		On suppose que $B_n \ne \emptyset, \forall n$. On note $B_n(y_1) := \Pi_1^{-1}(y_1) \cup B_n$, ce sont les éléments de $B_n$ qi commencent par $y_1$.
		
		Pour chaque $y_1$, $n \mapsto B_n(y_1)$ est décroissante. Comme $B_n = \underset {y_1 \in Y_1} \bigcup B_n(y_1)$ (union finie) (et $B_n \ne \emptyset$), il existe $y_1$ tel que les $B_n(y_1)$ sont tous non vides.
		
		On fixe maintenant un tel $y_1$ et on reprend le même raisonnement sur $y_2$, puis... On obtient de la sorte une suite $y$.
		
		Ainsi, il existe une suite $(y_1, \dots) \in B_n \forall n$ car $\forall n, \exists k_n, B_n\in\mathcal A_{k_n}$.
		
		Ainsi, $\forall n, B_n \ni y$ donc $\bigcap B_n \ne \emptyset$. Absurde.
	\end{itemize}
\end{demo}

\begin{propriete}
	Dans le contexte du théorème d'Hahn-Kolmogorov, $m^* : \mathcal P(X) \rightarrow [0,\infty]$ est une \emph{mesure extérieure}, c'est à dire que $m^*(\emptyset) = 0$, $m^*$ est croissante, et $m^*\left(\underset{i\in \mathbb N} \bigcup Z_i\right) \le \underset{i\in\mathbb N}\sum m^*(Z_i), \forall Z_i$.
\end{propriete}

\begin{demo}
	Démontrons la dernière propriété. Fixons $\varepsilon > 0$. Pour tout $i$, il existe un recouvrement $A_{i,j}, j\in \mathbb N$ de $Z_i$ tel que $\underset j \sum \underline m(A_{i,j}) \ge m^*(Z_i) \ge \underset j \sum \underline m (A_{i,j}) - \varepsilon 2^{-i}$, alors $A_{i,j}, i\in \mathbb N, j \in \mathbb N$ est un recouvrement de $\bigcup Z_i$, et $m^*(\bigcup Z_i) \le \underset {i,j} \sum \underline m (A_{i,j}) \le \underset {i \ge 1} \sum (m^*(Z_i) + \varepsilon 2^{-1}) \le \varepsilon + \underset {i \ge 1} \sum m^*(Z_i)$.
\end{demo}

\begin{demo}[Démonstration du théorème d'Hahn-Kolmogorov]
	Deux étapes : \begin{enumerate}
		\item $m^*|_{\mathcal A} = \underline m$
		Si $A \subset \underset i \bigcup A_i$, alors $\underline m (A) \le \sum \underline m (A_i)$ par $\sigma$-additivité de $\underline m$. En prenant l'inf, on obtient $\underline m(A) \le m^*(A)$. L'inégalité réciproque s'obtient en considérant le recouvrement trivial $A_1 = A, A_2=A_3 = \dots = \emptyset$.
		\item On dit que $Y \subset X$ est mesurable si, pour tout $\varepsilon > 0, \exists A \in \mathcal A$ tel que $m^*(Y \Delta A) \le \varepsilon$. Alors l'ensembre $\mathcal T$ des parties mesurables est une algèbre.
		\begin{demo}
			\begin{itemize}
				\item si $m^*(Y \Delta A) \le \varepsilon$, alors $m^*(\comp Y \cap \comp A) \le \varepsilon$, donc $\mathcal T$ est stable par complément.
				\item Soient $Y, Z$ mesurables et $A, B$ tels que $m^*(Y \Delta A) \le \varepsilon, m^*(Z \Delta B) \le \varepsilon$ alors $m^*((Y \cup Z) \Delta (A \cup B)) \le 2\varepsilon$ car $(Y \cup Z)  \Delta (A \cup B) \subset (Y \Delta A) \cup (Z \Delta B)$.
			\end{itemize}
		\end{demo}
		\item $m^*$ est une mesure additive sur $\mathcal T$.
		\begin{demo}
			$Y, Z$ disjoints, $A,B$ comme ci-dessus. 
			
			$(A\cap B) = (Y \cup (A \setminus Y)) \cap (Z \cup (B \setminus Z)) \subset Y \cap Z \cup (B \setminus Z) \cup (A \setminus Y)$ 
			
			donc $\underline m (A \cap B) \le 2\varepsilon$
			
			$A \cup B = (Y \cup (A \setminus Y)) \cup (Z \cup (B \setminus Z)) \subset Y \cup Z \cup (A \setminus Y) \cup (B \setminus Z)$
			
			$\underline m (A \cup B) \le m^*(Y \cup Z) +2\varepsilon$
			
			et $\underline m (A \cup B) = \underline m(A) + \underline m(B) - \underline m(A \cap B) \ge \underline m(A) + \underline m(B) -2\varepsilon \ge m^*(Y) - \varepsilon + m^*(Z) - \varepsilon - 2\varepsilon$.
			
			Finalement, $m^*(Y) + m^*(Z) \le m^*(Y \cup Z) + 6\varepsilon$ 
		\end{demo}
	Comme $m^*$ est une mesure extérieure et une mesure additive sur l'algèbre $\mathcal T$, elle a la propriété de $\sigma$-additivité.
	\item $\mathcal T$ est une tribu.
	\begin{demo}
		$Y_i \in \mathcal T$. On veut montrer que $Y_\infty := \underset i \bigcup Y_i \in \mathcal T$. On peut supposer que les $Y_i$ sont disjoints. Alors $\forall n, m^*(\underset{i = 1}{\overset{n} \bigsqcup} Y_i) = \underset{i = 1}{\overset{n} \sum} m^*(Y_i) \le m^*(X) = 1$. Donc la série $\sum m^*(Y_i)$ converge, donc $\forall \varepsilon, \exists n, \underset{i = n+1}{\overset{+\infty} \sum} m^*(Y_i) \le \varepsilon$.
					
		Alors en posant $Z= \underset{i = 1}{\overset{n} \bigcup} Y_i$, on a $m^*(Y_\infty \setminus Z) \le \varepsilon$, $Z \subset Y_\infty$. Ensuite, on prend $A \in \mathcal A$ tel que $m^*(A\setminus Z) \le \varepsilon, m^*(Z\setminus A) \le \varepsilon$. On obtient $A \setminus Y_\infty \subset A \setminus Z, Y_\infty \setminus A \subset (Z \setminus A) \cup (Y_\infty \setminus Z)$.
	\end{demo}
	\end{enumerate}
\end{demo}

\textbf{Complément :} on aurait pu donner une autre preuve du théorème 3 basée sur un résultat général sur les mesures extérieures. Lorsque $m^*$ est une mesure extérieure, on dit que $Y \subset X$ est $m^*$-mesurable si

$\forall Z \subset X, m^*(Z) = m^*(Z \cap Y) + m^*(Z \cap \comp Y)$.

\begin{theorem}[Carathéodory]
	Si $m^*$ est une mesure extérieure, l'ensemble $\mathcal T$ des parties $m^*$-mesurables est une tribu, et $m^*|_{\mathcal T}$ est une mesure.
\end{theorem}
\begin{rem}
	Dans le cas du théorème de Hahn, la tribu $\mathcal T$ est la même que celle introduite dans la démonstration précédente.
\end{rem}

\begin{demo}[Carathéodory $\Rightarrow$ Hahn-Kolmogorov]
	
	Il suffit de montrer que les éléments de $\mathcal A$ sont $m^*$-mesurables, et que $m^*|_{\mathcal A} = \underline m$.
	\begin{itemize}
		\item $m^*(A) \le \underline m(A) \forall A \in \mathcal A$
		\item $m^*(A) \ge \underline m (A) \forall A \in \mathcal A$. En effet, si $A \subset \underset i \bigcup A_i$, on peut supposer les $A_i$ disjoints. Alors par $\sigma$-additivité de $\underline m$ sur $\mathcal A$ : $\underline m(A) = \underset i \sum \underline m(A_i) \ge m^*(A)$.
		\item Soit $A \in \mathcal A$ et $Zin \mathcal P(X)$. On considère un recouvrement $A_i$ de $Z$.
		
		$\underset i \sum \underline m(A_i) = \underset i \sum \underline m (A_i \cap A) + \underline m(A_i \cap \comp A) \ge m^*(Z \cap A) + m^*(Z \cap \comp A)$.
		
		On prend l'inf : $m^*(Z) \ge m^*(Z \cap A) + m^*(Z \cap \comp A)$. L'autre inégalité découle de la sous-additivité.
	\end{itemize}
\end{demo}

\begin{demo}[Carathéodory]
	
	\begin{enumerate}
		\item $\mathcal T$ est une algèbre. 
		\begin{demo} On a $\emptyset \in \mathcal T, X \in \mathcal T$, et stabilité par complément de manière triviale.
		
		$A,B \in \mathcal T \Rightarrow \forall Y, m^*(Y) = m^*(Y\cap A) + m^*(Y \cap \comp A) = m^*(Y \cap A \cap B) + m^*(Y \cap A \cap \comp B) + m^*(Y \cap \comp A \cap \comp B) + m^*(Y \cap \comp A \cap B)$.
		
		\begin{rem}
			$\comp{(A \cap B)} = (\comp B \cap A) \cup (B \cap \comp A) \cup (\comp A \cap \comp B)$
		\end{rem}
		Donc $m^*(Y) \ge m^*(Y \cap (B \cup A)) + m^*(Y \cap \comp{(B \cap A)})$
		\end{demo}
		\item $m^*$ est additive sur $\mathcal T$
		\begin{demo}
			$A,B \in \mathcal T$, $A\cap B = \emptyset$.
			
			$m^*(A\cup B) = m^*((A\cup B)\cap A) + m^*((A \cup B) \cap \comp A) = m^*(A) + m^*(B)$
		\end{demo}
		\item $\mathcal T$ est une tribu.
		\begin{demo}
			soit $A_n$ une suite d'éléments deux à deux disjoints de $\mathcal T$. Posons $B_n =\bigcup_{k=1}^n A_n$ et $B_\infty = \bigcup_{k=1}^\infty A_n$.
			
			$\forall Y \subset X, m^*(Y \cap B_n) = m^*(Y \cap B_n \cap A_n) + m^*(Y \cap B_n \cap \comp{A_n}) = m^*(Y \cap A_n) + m^*(Y \cap B_{n-1})$.
			
			Donc $m^*(Y\cap B_n) = \sum_{k=1}^n m^*(Y \cap A_k)$.
			
			Alors $m^*(Y) = m^*(Y) = m^*(Y \cap B_n) + m^*(Y \cap \comp{B_n}) \ge \sum_{k=1}^n m^*(Y \cap A_k) + m^*(Y \cap \comp{B_{infty}})$.
			
			À la limite : $m^*(Y) \ge \sum_{n=1}^\infty m^*(Y \cap A_n) + m^*(Y \cap \comp{B_{\infty}})$
		\end{demo}
	\end{enumerate}
\end{demo}

On peut cependant se poser la question de l'unicité de $m^*$ dans Hahn-Kolmogorov.

\begin{theorem}
	Si $\mu : B \rightarrow [0,1]$ est une mesure sur une tribu $B \subset \A$, $\mu|_\A = \underline m$, alors $\mu=m^*$ sur $\T \cap B$.
\end{theorem}

\begin{rem}
	Il existe une plus petite tribu contenant $\A$ ($\underset{\T \subset \A,\ \T \text{ tribu}}{\bigcap} \T$). Sur cette tribu, il existe une unique mesure prolongeant $\underline m$.
\end{rem}

\begin{demo}
	\begin{enumerate}
		\item Si $B \subset \underset i \bigcup A_i, A_i \in \A_i, B \in \B$, alors $\mu(B) \le \underset i \sum \mu(A_i) = \underset i \sum \underline m(A_i)$. En prenant l'inf sur les familles $A_i$, on conclut $\mu \le m^*|_\B$
		\item Comme $\mu(B) \le 1- \mu(\comp B)$, si $B \in \T$, on a $A-m^*(\comp B) = m^*(B)$ donc $\mu(b) \ge m^*(B)$ et donc $\mu(B) = m^*(B) si B\in \T$
	
	\end{enumerate}
\end{demo}

\section{Loi des grands nombres}

On se donne $Y \subset \R$ fini, une mesure de probabilité $p$ sur $Y$, et une suite finie $f_{i,i\in\N} : \Omega \rightarrow Y$ de variables aléatoires iid suivant la loi $p$. L'existence d'une telle suite découle des théorèmes de la section précédente.

\begin{definition}
	Si $f : \Omega \rightarrow \R$ est une variable aléatoire prenant un nombre fini de valeurs, on note $E(f) = \underset{y \in f(\Omega)} \sum yP(f=y)$ l'espérance de $f$.
\end{definition}

Dans notre contexte on note $e := E(f)$.

On définit $S_n = \frac{f_1 + \dots + f_n} n : \Omega \rightarrow \R$. Chacune des variables aléatoires $S_n$ prend un nombre fini de valeurs, mais les variables $S_n$ ne sont pas indépendantes.

On veut montrer les trois énoncés suivants :

\begin{theorem}[Loi faible des grands nombres]
	\[P\left(\left| \frac{S_n} n -e \right| \ge \eps\right) \rightarrow 0\]
\end{theorem}

\begin{theorem}[Loi forte des grands nombres]
	\[P \left(\frac {S_n} n \rightarrow e\right) = 1\]
\end{theorem}

\begin{theorem}
	\[\forall \alpha > \frac 1 2, \quad P\left(\frac {S_n - ne} {n^\alpha} \rightarrow 0\right) = 1\]
\end{theorem}

\subsection{Quelques outils de théorie de la probabilité}

Pour démontrer ces résultats, on va avoir besoin d'un certain nombre d'autres outils.

\begin{theorem}[Inégalité de Markov]
	Si $f$ est une variable aléatoire positive, 
	\[ \forall a \in \R^+_*, \quad P(f > a) \le \frac {E(f)} a \]
\end{theorem}

\begin{demo}
	On écrit la définition de $E(f)$, on coupe la somme en deux selon $y>a$ ou $y \le a$, on majore brutalement et on conclut.
\end{demo}

\begin{theorem}[Inégalité de Bienaymé-Tchebychev]
	\[ \forall a \in \R^+_*, \quad P(|f-E(f))| > a) \le \frac {\var(f)} {a^2} \]
\end{theorem}

\begin{demo}
	On élève l'événement au carré, on conclut par Markov.
\end{demo}

\begin{propriete}
	$E(XY) := E(X)E(Y) + \cov (X,Y)$,
	
	$\var(X+Y) = \var(X) + \var(Y) + 2\cov (X,Y)$
\end{propriete}

\begin{demo}
	Il suffit de l'écrire.
\end{demo}

\begin{lemme}
	Si $X$ et $Y$ sont deux variables aléatoires indépendantes, $\cov(X,Y) = 0$.
\end{lemme}
\begin{demo}
	Trivial.
\end{demo}

\begin{propriete}[Convergence monotone]
	Soit $(\Omega, \T, m)$ un espace mesuré.
	
	Si $(A_n)$ est une suite décroissante et que $m(A_1)$ est fini, alors $m(\underset {n\in\N} \bigcap A_n) = \lim m(A_n)$.
	
	Si $(A_n)$ est une suite croissante, alors $m(\underset {n\in\N} \bigcup A_n) = \lim m(A_n)$.
\end{propriete}

\begin{demo}
	On pose $B_n = A_n \setminus A_{n-1}$, et par double passage à la limite, la propriété sur les suites croissantes est immédiate. Le résultat sur les suites décroissantes vient du passage au complémentaire.
\end{demo}

\begin{lemme}[Fatou ensembliste]
	\begin{itemize}
		\item $m(\liminf A_n) \le \liminf m(A_n)$
		\item Si $m$ est finie, $m(\limsup A_n) \ge \limsup m(A_n)$
		\item Si $m$ est finie et $\limsup A_n = \liminf A_n = A$, alors $m(A_n) \rightarrow A$
	\end{itemize}
\end{lemme}

\begin{demo}
	On pose $B_n = \underset {m \ge n} \bigcap A_m$. C'est une suite croissante.
	
	$m(B_n) \rightarrow m(\underset n \bigcup B_n) = m(\liminf A_n)$
	
	$m(B_n) \le m(A_n)= \Rightarrow \liminf m(A_n) \le m(\liminf A_n)$
\end{demo}

\begin{lemme}[Premier lemme de Borel-Cantelli]
	Si $\underset {n\ge 0} \sum m(A_n)$ est finie, alors $m(\limsup A_n) = 0$.
\end{lemme}

\begin{demo}
	$B_n := \underset {m\ge n} A_m$.
	
	$m(B_n) \le \sum_{m \ge n}^\infty m(A_n) \rightarrow 0$ (reste de série convergente)
	
	Or, $\lim m(B_n) = m(\limsup A_n) = 0$.
\end{demo}

\begin{theorem}[Inégalité de Kolmogorov]
	$P( \underset {A \le k \ le n} \max |\tilde S_k| \ge a) \le \frac {n \var(f)} {a^2}$
\end{theorem}

\begin{demo}
	$T(\omega) :=$ le premier temps pour lequel $|\tilde S_n \ge a$. $T(\omega) \in \N \cup \{+\infty\}$
	
	$(T = k) = \{|\tilde S_1| < a \} \cap \dots \cap \{|\tilde S_{n-1}| < a \} \cap \{|\tilde S_n| \ge a \} \in \A_k$.
	
	$T$ est ainsi un \emph{temps d'arrêt}.
	
	$\begin{aligned}
		\var \tilde S_n &= E(\tilde S_n^2) \ge \sum_{k=1}^n E(S_n^2 \mathds 1_{\{T=k\}}) \\
		&= \sum_{k=1}^n E((\tilde S_n + \tilde S_k - \tilde S_k) \mathds 1_{\{T=k\}}) \\
		&= \sum_{k=1}^n E(\tilde S_k^2 \mathds 1_{\{T=k\}}) + \sum_{k=1}^n E((\tilde S_n - \tilde S_k)\tilde S_k \mathds 1_{\{T=k\}}) \\
		&\ge \sum_{k=1}^n a^2P(T=k) + 0 + 0 (*) \\
		&\ge a^2 P(M_n \ge a)
	\end{aligned}$
et $(*) : \tilde S_n - \tilde S_k = \tilde f_{k+1} + \dots + \tilde f_n$. Or $\tilde S_k \mathds 1_{\{T=k\}}$ ne dépend que des $k$ premières valeurs (indépendance).
\end{demo}

\begin{rem}
	Illustration de la notion de temps d'arrêt :
	
	On considère un jeu de hasard : une suite $f_i$ de v.a. i.i.d. à valeurs dans $\{-1, 1\}$, avec $P(1) =p$.
	
	Supposons que le joueur choisit un temps $T(\omega)$ pour miser. Peut-il optimiser sa probabilité de gain $P(f_{T(\omega)}(\omega) = 1)$ ?
	
	On peut choisir $T(\omega)$ le premier temps tel que $f_{T(\omega)} = 1$, mais cela nécessite de connaître tous les tirages.
	
	En réalité, on ne dispose pas de l'almanach des sports, on n'a que l'information des $k-1$ premiers tirages, i.e. $\{T=k\} \in \A_{k-1}$, c'est un temps d'arrêt.
	
	\begin{propriete}
		Si $T$ vérifie cette condition, $P(f_{T(\omega)}(\omega)=1) = p$.
	\end{propriete}
	\begin{demo}
		$P(f_{T(\omega)}(\omega)=1) = \sum_{k=1}^\infty P((T=k) \cap (f_k =1))$. On conclut par indépendance.
	\end{demo}
\end{rem}

\begin{theorem}[Inégalité de Hoeffding]
	\[ P\left( \left| \tilde S_n \right| \ge a \right) \le 2 \exp (-\frac{2a^2}{Cn}), \qquad C = (\max f - \min f)^2\]
\end{theorem}

\begin{lemme}
	Pour $\tilde f$ une v.a. centrée prenant un nombre fini de valeurs, on a : 
	\[ E\left( e^{\theta \tilde f} \right) \le e^{C\theta^2/8}, \qquad C=(\max \tilde f - \min \tilde f)^2\]
\end{lemme}

\begin{demo}[lemme]
	On pose $g(\theta):=\ln(E(e^{\theta\tilde f}))$. $g$ est $\mathcal C^\infty$.
	
	On a : $g(0) = 0$, $g'(\theta) = \frac{E(\tilde f e^{\theta\tilde f})}{E(e^{\theta \tilde f})}$ donc $g'(0) = E(\tilde f) = 0$.
	
	Au voisinage de $\theta = 0$, il existe une constance $c$ telle que $g(\theta) \le c\theta^2$.
	
	$g''(\theta) = \frac{E(\tilde f^2 e^{\theta \tilde f}) E(e^{\theta\tilde f}) - E(\tilde f e^{\theta\tilde f})^2} {E(\theta e^{\theta\tilde f})^2}$
	
	Ceci est la variance de la loi de proba sur $\tilde Y$ donnée par $P_\theta(\tilde y) = \frac{P(\tilde y)e^{\theta\tilde f}}{E(e^{\theta \tilde f})}$.
	
	En effet, $g''(\theta) = E \left(  \frac{\tilde f^2 e^{\theta\tilde f}} {E(e^{\theta \tilde f})} \right) - E \left( \tilde f \frac{e^{\theta\tilde f}}{E(e^{\theta\tilde f})} \right)^2$.
	
	Si $g$ est une v.a. prenant un nombre fini de valeurs, alors $\var(g) \le (\max g - \min g)^2/4$. On remarque que la variance est invariante à translation de $g$ près, donc on peut supposer $\max g = - \min g$. Or, $\var(g) = E(g^2) - E(g)^2 \le E(g^2) \le (\max g)^2 \le (\max g - \min g)^2/4$.
	
	Ainsi, par l'inégalité des accroissements finis, comme $g''(\theta) \le \frac C 4$, on a $g(\theta) \le \frac C 8 \theta^2$
	
	Donc $E(e^{\theta\tilde f}) \le \exp\left( \frac{C\theta^2} 8 \right)$
\end{demo}

\begin{demo}[Hoeffding]
	$P(\tilde S_n \ge a) = P(e^{theta\tilde S_n} \ge e^{\theta a}) \le e^{-a\theta}E(e^{\theta\tilde S_n})$ par Markov.
	
	$E(e^{\theta\tilde S_n}) = E(\prod e^{\theta\tilde f_i}) = \prod E(e^{\theta\tilde f_i}) = E(e^{\theta\tilde f})^n$.
	
	$P(\tilde S_n \ge a) \le e^{-a\theta} E(e^{\theta\tilde f})^n \le \exp \left(\frac {nC\theta^2} 8 - \theta a \right)$.
	
	On optimise par rapport à $\theta$ ($\theta = \frac{4a}{nC}$), et on conclut.
\end{demo}

\begin{rem}
	\textbf{Intérêt de ces inégalités en statistiques}
	
	Bienaymé-Tchebychev : $P(|\tilde S_n| \ge a) \le \frac{n\var(f)}{a^2}$
	
	Hoeffding : $P(|\tilde S_n| \ge a) \le 2\exp(-\frac{2a^2}{nC})$
	
	Dans les deux cas, on note une décroissance en $\frac n {a^2}$.

	Exemple d'application : sondage. La population peut avoir deux avis : $Y = \{ 0, 1\}$. On cherche à estimer par un sondage quelle est la proportion $p$ de la population qui a l'avis $1$ ($p=P(1)$).
	
	On interprète un sondage comme étant une suite finie de variable aléatoires iid $f_1, \dots, f_n$ tirées suivant la loi ci-dessus.
	
	On s'attend à ce que $\frac {S_n} n \approx p$. Les inégalités rappelées ci-dessus nous donnent des majorations de $P\left(\left|\frac{S_n} n - p \right| \ge \eps \right)$. Dans ce contexte, Hoeffding donne de meilleures estimations.
	
	Exemple de valeurs numériques :
	\begin{center}
		\begin{tabular}{|c|c||c|}
			\hline
			n & $\eps$ & Résultat \\
			\hline
			1000 & 5\% & $p=1,3$\% \\
			\hline
			1000 & 1\% & $n\eps^2 =1$, on ne peut rien conclure \\
			\hline
		\end{tabular}
	\end{center}

	La première ligne indique que la probabilité d'être à plus de 5\% d'erreur est d'au plus 1,3\%.
	
	\textbf{Intervalle de confiance :}
	
	Ici, on fixe $p$ la probabilité d'erreur, et on cherche $\eps$, c'est à dire, par Hoeffding, $\eps \le \sqrt{\frac 1 {2n} \ln\left(\frac 2 p \right)}$. Par exemple, pour $n=1000$, $p=5\%$, on obtient $\eps=4,"\%$.
	
	En général, avec ces données, on donne $\eps = 3\%$. Cette disparité vient de la non-optimalité de l'inégalité de Hoeffding, et par le fait qu'il existe des modèles plus précis (approcher cette binomiale par une gaussienne grâce au théorème central limite par exemple).
\end{rem}

\subsection{Démonstrations des lois des grands nombres}

\begin{demo}[Loi faible des grands nombres]
	
	$P(\left|\frac {S_n} n - e \right| \ge \alpha) = P(|S_n - E(S_n)| \ge n\alpha) \le \frac{\var(S_n)}{n^2\alpha^2} = \frac{n \var(f)}{n^2\alpha^2} = \frac{\var(f)}{n\alpha^2}$
\end{demo}

\begin{demo}[Loi forte des grands nombres]
	$\sum_n P(A_{n^2}(\eps))$ converge.
	
	Donc $m(\limsup(A_{n^2}(\eps)))= 0$ d'après Borel-Cantelli. Donc $\frac {S_{n^2}} {n^2} \rightarrow E(f)$ p.p.
	
	Montrons alors que si $\frac {S_{n^2}} {n^2} \rightarrow E(f)$ alors $\frac {S_n} n \rightarrow E(f)$.
	
	On note $M = \max |f|$. Soit $k(n)$ tel que $k(n)^2 \le n < (k(n) + 1)^2$.
	
	\begin{align*}
		\left| \frac{S_n - nE(f)} n \right| &\le \frac{|S_{k(n)^2} - k(n)^2E(f)| + (n-k(n)^2)(M+E(f))}{k(n)^2} \\
		&\le \left| \frac {S_{k(n)^2} - k(n)^2E(f)}{k(n)^2} \right| + \frac{(k(n)^2 +1) - k(n)^2}{k(n)^2} (M + E(f))
	\end{align*}
	Chacun des termes tend vers $0$, ce qui achève la preuve.
\end{demo}

\begin{demo}[Inégalité de Kolmogorov $\Rightarrow$ théorème 3]
	
	$P(\frac{M_n}{n^\alpha} \ge \eps) \le \frac{\var(f)}{n^{2\alpha -1}\eps^2}$ où $M_n = \underset{1\le k \le n}\max |\tilde S_k|$
	
	On fixe $R\in\N$ tel que $(2\alpha -1)r > 1$.
	
	\[P(\frac{M_{n^r}}{n^{r\alpha}} \ge \eps) \le \frac{\var(f)}{\eps^2 n^{(2\alpha - 1)r}}\]
	
	C'est le terme général d'une série convergente, donc par le premier lemme de Borel-Cantelli, on en déduit que $P(\frac{M_{n^r}}{n^{r\alpha}} \ge \eps \text{ i.o.}) = 0$, et donc que p.p., $\frac{M_{n^r}}{n^{r\alpha}} \rightarrow 0$.
	
	Mais $\frac{M_{n^rr}}{n^{r\alpha}} \rightarrow 0 \Rightarrow \frac{\tilde S_n}{n^\alpha} \rightarrow 0$.
	
	En effet, soit $k(n)$ tel que $(k(n) -1)^r \le n < k(n)^r$. Alors,
	\begin{align*}
		\frac{\tilde S_n}{n^\alpha} &\le \frac{M_{k(n)^r}}{n^\alpha} \\
		& = \frac{M_{k(n)^r}}{k(n)^{\alpha r}} \cdot \frac{k(n)^{\alpha r}}{n^\alpha} \\
		&\le \frac{M_{k(n)^r}}{k(n)^{\alpha r}} \cdot \frac{k(n)^{\alpha r}}{(k(n) -1)^{\alpha r}}
	\end{align*}
	Le premier terme tend vers 0, le second vers 1, ce qui conclut la preuve.
\end{demo}

\begin{demo}[Inégalité de Hoeffding $\Rightarrow$ théorème 3]
	$P(|\tilde S_n| \ge \eps n^\alpha) \le 2\exp \left( \frac{-2\eps^2n^{2\alpha -1}}{C} \right)$
	
	C'est le terme général d'une série convergente, donc d'après le 1er lemme de Borel-Cantelli :
	
	$P(|\tilde S_n| \ge \eps n^\alpha \text{ i.o.})$ ie $\frac{\tilde S_n}{n^\alpha} \rightarrow 0 \text{ p.p.}$.
\end{demo}

\section{Estimations inférieures}

On suppose que les variables aléatoires prennent au moins deux valeurs avec probabilité non nulle.

\subsection{Quelques estimations inférieures}

\begin{theorem}
	\[ P((S_n) \text{ bornée} ) = 0\]
\end{theorem}

\begin{theorem}
	\[ P(\limsup \frac{\tilde S_n}{\sqrt n} = +\infty ) = 1 \]
\end{theorem}

\begin{theorem}[Loi du logarithme itéré]
	Presque partout, on a:
	\[ \limsup \frac{\tilde S_n}{\sqrt{2(\var(f))n \ln \ln n}} = 1\]
\end{theorem}

\subsection{Quelques résultats utiles}

\begin{lemme}[2nd lemme de Borel-Cantelli]
	Si $(A_n)$ est une suite d'événements indépendants, telle que $\underset n \sum P(A_n) = + \infty$, alors $P(\limsup A_n) = 1$.
\end{lemme}

\begin{rem}
	Si $(A_n)$ est une suite d'événements indépendants, alors d'après les deux lemmes de Borel-Cantelli : $P(\limsup A_n) \in \{0,1\}$.
\end{rem}

\begin{demo}[Second lemme de Borel-Cantelli]
	\begin{align*}
		&B_n := \{f_n=a, \dots, f_{n+k}=a\} \\
		&P(B_n) =\underset {\sum = +\infty} {\underbrace{P(f=a)^k}} \\
		\text{Par BC2 ($(B_n)$ indépendants), } &P(B_n \text{ i.o.}) = 1 \\
		\text{Donc } &P(B_n \text{ i.o.}) = 1
	\end{align*}
\end{demo}

\begin{lemme}
	$\forall p, \exists C, \forall n, P_p(S_n=k)\ge \frac 1 {C\sqrt n} \exp{\left( -nC\left| \frac nk n -p\right|^2 \right)}$
\end{lemme}

\begin{demo}
	On cherche en fait à faire une estimation de la loi binomiale $P_p(S_n=k) = \frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}$
	
	$\frac{P_p(S_n=k)}{P_{\frac n k}(S_n=k)} = \frac{p^k(1-p)^{n-k}}{\left(\frac n k\right)^k \left(1- \frac n k\right)^{n-k}} = \exp{\left( -nh\left(\frac k n, p\right)\right)}$ où $h(s,p) = s\ln\left(\frac s p \right) +(1-s)\ln\left(\frac{1-s}{1-p}\right)$.
	
	En particulier, comme $h$ est $\mathcal C^2$ sur un compact, $h(s,p) \le C|s-p|^2$ (à $p$ fixé).
	
	$P_p(S_n=k) \ge P_{\frac k n}(S_n=k)\exp\left( -n \left| \frac k n -p\right|^2\right)$
	
	Il reste à estimer $P_{\frac k n}(S_n=k) = \frac{n!}{k!(n-k)!}\left(\frac k n\right)^k \left( \frac {n-k} n \right)^{n-k}$.
	
	On conclut par Striling.
\end{demo}

\begin{propriete}
	$\forall A > 0, \exists \delta > 0, \forall n\in\N, P(\tilde S_n \ge A\sqrt n) \ge \delta$
\end{propriete}

\begin{demo}
	
	$\begin{aligned}
	P(\tilde S_n \ge A\sqrt n) \overset {\tilde S_n = S_n - np} =& \underset{k\ge np + A\sqrt n} \sum P(S_n=k) \\
	\ge & \underset{np + A\sqrt n \le k \le np +(A+\frac 1 2) \sqrt n} \sum P(S_n=k) \\
	\ge & \sqrt n \frac 1 {C\sqrt n} \exp \left(-nC \frac {(A+\frac 1 2)^2} n \right) \\
	=& \underset{\delta} {\underbrace{\frac 1 C \exp\left( -C(A + \frac 1 2)^2 \right)}}
	\end{aligned}$
\end{demo}

\subsection{Démonstration des théorèmes 13-15}

\begin{demo}[Théorème 13]
	On se donne $a\ne 0$, $P(a) > 0$.
	 
	\textbf{\quad Affirmation :} Pour tout $k\in\N$, il existe une infinité de $n\in\N$ tels que $f_n = a, \dots, f_{n+k} = a$.
	\begin{demo}
		On démontre cette affirmation en passant au complémentaire.
		
		$B_n := \comp{A_n}$. Les $(B_n)$ sont indépendants.
		
		On veut montrer que $P(\liminf B_n) = 0$ i.e. $P(B_n \text{ APCR}) = 0$.
		
		Or, $\liminf B_n = \underset {n\in\N} \bigcup \  \underset{m\ge n}\bigcap B_m$. Il suffit alors de montrer que $P(\underset{m\ge n}\bigcup B_m) = 0$.
		
		$P(\underset{m=n}{\overset l \bigcap} B_m) \overset{\indep} = \underset{m=n} {\overset l \prod} P(B_m) = \underset{m=n}{\overset l \prod}(1-P(A_m)) \overset{1-x \le e^{-x}} \le e^{-\underset{m=n}{\overset l \sum} P(A_m)} \underset{l\rightarrow +\infty} \longrightarrow 0$.
		
		Donc $P(\underset{m\ge n} \bigcap B_m) = 0$.
	\end{demo}
	
	On en déduit immédiatement le caractère non borné de la suite $(S_n)$
\end{demo}

\begin{demo}[Théorème 14]
	On suppose $A$ "assez grand" (dans un sens qui se précisera dans le corps de la preuve).
	
	On considère la sous-suite $n_k$ telle que $n_1=1$ et $n_{k+1}=n_k+4n_k^2$.
	
	$P\left( \left|\tilde S_{n_{k+1}} - \tilde S_{n_k} \right| \ge a \right) = P\left( \left|\tilde S_{n_{k+1} - n_k} \right| \ge a\right)$.
	
	On considère les événements $B_k = \left\{ \tilde S_{n_{k+1}} - \tilde S_{n_k} \ge A\sqrt{n_{k+1} - n_k} \right\} = \left\{ \tilde S_{4n_k} \ge 2An_k \right\}$.
	
	$B_k$ dépend de $f_{n_k+1} \dots f_{n_{k+1}}$ donc les $(B_k)$ sont indépendants.
	
	Comme $P(B_k) \ge \delta$, d'après BC2, $P(\limsup B_k) = 1$.
	
	Pour un point $\omega \in B_k \text{ i.o.}$, on a donc une infinité de $k$ tels que
	
	$\begin{aligned}
		\tilde S_{n_{k+1}} &\ge \tilde S_{n_k} +2An_k \\
		&\ge n_k(2A+\min f)
		(\text{pour } A \ge |min f|) &\ge An_k
		\end{aligned}$
	
	Or, $n_{k+1} \le 16n_k^2$ donc $n_k \ge \frac {\sqrt{n_{k+1}}} 4$. Donc $\tilde S_{n_{k+1}} \ge \frac A 4 \sqrt{n_{k+1}}$ pour une infinité de $k$. Donc $\limsup \frac {\tilde S_n} {\sqrt n} \ge \frac A 4$. Donc $P(\limsup \frac{\tilde S_n}{\sqrt n} = \infty) = 1$.
\end{demo}

\subsection{Quelques extensions}

On a montré avec le second lemme de Borel-Cantelli l'existence de séquences qui apparaissent presque partout. Il se peut malgré tout que le temps d'attente soit très long.

\begin{lemme}[Estimation du temps d'attente]
	$a\in \Im f, P(a) > 0$. D'après le second lemme de Borel-Cantelli, la valeur $a$ est prise une infinité de fois.
	
	Soit $T$ le premier temps pour lequel $f_T=a$. Alors $E(T) = \frac 1 {P(a)}$
\end{lemme}

\begin{demo}
	On montre facilement que $T$ suit une loi géométrique, et en utilisant $E(T) = \underset{k\ge 1} \sum P(T\ge k)$, on conclut rapidement.
\end{demo}

\chapter{Intégration}

\section{Quelques remarques sur les tribus}

On a déjà défini la tribu la plus petite possible. 

\begin{definition}
Si $\mathcal{B} \subset \mathcal{P} (X)$, on appelle tribu engendrée par $\mathcal{B}$ la plus petite tribu contenu $\mathcal{B}$.
\end{definition}

\begin{lemme}
Elle existe car une intersection de tribus est une tribu.
\end{lemme}

\begin{demo}
Soit $\A_{i, i \in I}$ des tribus sur X. Alors $A = \underset{i \in I} \bigcap \A_i $ est une tribu.

Soit en effet $(A_n) \in \A^\N$, alors $\forall i, \forall n, \quad A_n \in \A_i $, donc pour tout $i$, l'union des $A_n$ appartient à $A_i$. Donc l'union, des $A_n$ appartient à l'intersection des $\A_i$.
\end{demo}

Avec ce vocabulaire, on a le théorème :

\begin{theorem}
Soit $\A$ une algèbre des parties de X et $\underline{m}$ une mesure de probabilités additive sur $\A$, qui de plus est $\sigma$-additive. Alors, il existe une unique mesure de probabilité $m$ sur $\tau$, la tribu engendrée par $\A$.
\end{theorem}

On peut, de la même façon, définir l'algèbre engendrée par $\B$.  L'algèbre engendré est inclus dans la tribu engendrée.

On peut définir l'algèbre engendrée de façon plus explicite : on note $\B' = \lbrace B \subset X ; B \in \B \text{ ou } \comp B \in \B \rbrace $. L'algèbre engendrée est l'ensemble des unions finies d'intersections finies d'éléments de $\B'$. Il suffit en effet de vérifier que cette famille d'ensemble est stable par union, intersection et complémentaire. 
\begin{itemize}
	\item La stabilité par union est tautologique.
	\item De même pour la stabilité par complémentaire une fois qu'on a mointré la stabilité par intersection.
\end{itemize}

Montrons la stabilité par intersection : 

\begin{demo}
$A = \bigcup _ i \bigcap _j B_{ij} $, $\tilde A  = \bigcup_k \bigcap _ l \tilde B_{kl} $ et 

\begin{align*}
A \bigcap A &= (\bigcup _i \varhexstar ) ( \cup _ k \varhexstar ) \\
&= \bigcup_{i,k} \left( \left( \bigcap_j B_{ij} \right) \cap \left( \bigcap_l B_{kl} \right) \right) \\
&= \bigcup _{i,k} \left( \bigcap_{j,l} B_{ij} \cap B_{kl} \right) 
\end{align*}

est une union finie d'intersections finies donc le complémentaire de l'union des intersections des $B_{ij}$ qui est l'intersection de l'union des complémentaires des $B_{ij}$ est une intersection finie d'éléments de notre ensemble et donc est dedans.
\end{demo}

Il est plus difficile de décrire les éléments de la tribu engendrée, notamment car l'ensemble des unions dénombrables d'intersections dénombrables n'est pas stable par intersection dénombrable. 

\begin{propriete}
Soit $f:(\Omega,\T) \rightarrow X$ une application et soit $\B \subset \T $ engendrant $\T$. \\
Si $f^{-1} (B)\in \T , \forall B \in \B$ , alors $f$ est mesurable.
\end{propriete}

\begin{demo}
On considère l'ensemble $\eps$ des partitions de $X$ dont la pré-image par $f$ est mesurable. $ \eps = \lbrace Y \in \mathcal P (X), \ f^{-1}(Y) \in \T \rbrace $. 
On a déjà démontré que c'est une tribu (notée $f_* \T$. Cette tribu contient $\B$, donc elle contient $\T$. 

\end{demo}

\begin{propriete}
Les parties de $\R$ suivantes engendrent la même tribu.
	\begin{enumerate}
		\item Les intervalle ;
		\item Les intervalles fermées ;
		\item Les intervalles ouverts ;
		\item les ouverts ;
		\item Les fermées ;
		\item Les intervalles $] - \infty , a ], a \in \mathbb{D} $ (ou n'importe quel ensemble dense dans $\R$).
	\end{enumerate}
On l'appelle la tribu Borélienne, et on la note $\borel(\R)$.

\end{propriete}

\begin{demo}

On note $\T_i $ la tribu engendrée par (1).

La tribu engendrée par les ouverts contient les fermés, donc $\T_5 \subset \T_4 $ et de même, nous avons l'autre inclusion, donc égalité. 

L'inclusion de $\T_2 $ dans $\T_1$ est claire. Montrons que tout intervalle est dans $\T_2$. Tout intervalle est une réunion dénombrable d'intervalles fermés, donc appartient à $\T_2$.

$\T_3 = \T_1$ car tout intervalle est une intersection dénombrable d'intervalles ouverts. Réciproquement, tout ouvert est une union dénombrable d'intervalles ouvrets.

$\forall a < b, [a,b] = \bigcap_n ]-\infty, b_n ] \setminus \bigcup_n ]-\infty, a_n ]$ où $b_n$ tend en décroissant vers $b$ et $a_n$ tend en croissant vers $a$ sans jamais l'atteindre. 
\end{demo}

\begin{propriete}
Sur $[0, 1[$, les trois tribus suivantes sont les mêmes : 
\begin{enumerate}
\item La tribu engendrée par les intervalles ;
\item La tribu engendrée par les $[0, a], a \in \mathbb{D} \cap [0,1[ $ ;
\item La tribu $\lbrace B \cap [0,1[,\ B \in \borel(\R) \rbrace = \iota^*(\borel(\R)) $ où $\iota:[0,1[\rightarrow \R$ est l'inclusion ($\iota(A) = A\cap [0,1[$).
\end{enumerate}

\end{propriete}

\begin{definition}
Soit $f_i : \Omega \rightarrow (X_i , \T_i ) , i \in I$ une famille d'applications.

La tribu engendrée par les $(f_i)_{ i \in I }$ est la tribu de $\Omega$ engendrée par l'union sur $i \in I$ des $f_i^*(\T_i ) $ . C'est la plus petite tribu pour laquelle toutes les applications $f_i$ sont mesurables.
\end{definition}

\begin{definition}[Tribu produit]

Soit $(X_i , \T_i )$ des espaces mesurables, $i \in I $.

La tribu produit sur $\Omega := \prod_{i\in I} X_i $ est la tribu engendrée par les projections de $\Omega$ sur les $X_i$.

\end{definition}

Autrement dit, c'est par définition la tribu engendrée par les cylindres $Z_i \times \prod_{j \neq i} X_j $, avec $Z_i \in \T_i $. 


\begin{rem}

Si $I$ est fini ou dénombrable, la tribu produit est la tribu engendrée par le produit des $Z_i$. 

\end{rem}

\begin{rem}
	
Si $I$ n'est pas dénombrable, les éléments de la tribu produit dépendent d'une famille au plus dénombrable de coordonnées, c'est à dire que si $A \in \T$, alors il existe une partie $J \subset I$ telle que $I \setminus J$ soit au plus dénombrable et telle que $\pi_J : \prod_I X_i \rightarrow \prod_J X_j$ vérifie $\pi_J(A) =\prod_J X_j$, autrement dit, $A$ ne dépend pas des coordonnées de $J$.
\end{rem}

\begin{propriete}

Si $f : (X_i , \T_i ) \rightarrow \Omega$ et $F : \Omega \rightarrow ( \prod X_i , \prod \T _i ) $ l'application dont les $f_i$ sont les coordonnées, alors la tribu engendrée par les $f_i$ est $F^*(\prod \T_i ) $.

\end{propriete}

\begin{propriete}

L'application $F : ( \Omega , \T ) \rightarrow ( \prod X_i , \prod \T_i ) $ est mesurable si et seulement si ses coordonnées le sont.

\end{propriete}


\subsection{Tribus de $\R ^d$}

$d \geq 1 $ \\

\begin{theorem}
Les tribus suivantes sont identiques :
\begin{enumerate}
\item Les tribus engendrées par les fermées ;
\item Les tribus engendrées par les ouverts ;
\item Le produit des tribus boréliennes ;
\item La tribu engendrée par les produits d'intervalles ;
\item La tribu engendrée par les produits des $]-\infty , a_i ], a_i \in \mathbb{D} $ (ou n'importe quel ensemble dense).
\item La tribu engendrée par les produits d'intervalles ouverts. 
\end{enumerate}

On l'appelle la tribu borélienne de $\R ^d $.

\end{theorem}

\begin{rem}

C'est un cas particulier de la tribu produit et c'est un cas particulier de tribu borélienne. Dans le cas général, sur un espace métrique, on appelle tribu borélienne la tribu engendrée par les ouverts.
$\T_1 = \T_2$ est toujours vrai.

\end{rem}

\begin{demo}

$\T_3$ est la tribu engendrée par les produits de boréliens, donc $\T_3 \subset T_4 \subset T_5$, mais $(\pi_i)^*\T_5$ contient les intervalles $]-\infty, a], a\in\mathbb D$, donc contient la tribu borélienne.

En conséquence, $\pi_i^{-1}(Z_i)$ est dans $\T_5$ pour tout $Z_i$ mesurable, donc $\T_5$ est la tribu produit. Donc $\T_5 = \T_4=\T_3$, et c'est aussi la tribu engendrée par les produits d'intervalles ouvertes ($\T_6$).

Donc $\T_6 \subset \T_1$, et réciproquement. Donc $\T_6=\T$, car tout ouvert est une réunion dénombrable de pavés ouverts.
\end{demo}

\section{La mesure de Lebesgue sur $\R$}

Soit $\B := \borel(\R)$.

\begin{theorem}
Il existe une unique mesure $\lambda : \B \rightarrow [0, + \infty ] $ telle que $\lambda ( [a,b] )= b-a \quad \forall a \leq b $.
\end{theorem} 

C'est la mesure de Lebesgue. 

\begin{rem}
$\lambda$ n'est pas une mesure finie. Mais elle est $\sigma$-finie : on peut recouvrir $\R$ par un nombre dénombrable d'ensemble de mesure finie, par exemple $[-n;n] $, ou $[n,n+1]$.
\end{rem}

On va commencer par construire la mesure de Lebesgue sur $[0, 1]$. 

\begin{theorem}
Il existe une unique mesure borélienne $\lambda : \B \rightarrow [0, 1] $ telle que $\lambda ( [a,b]) = b-a \quad \forall a \leq b $ dans $[0;1]$..
\end{theorem}

\begin{demo}

On considère l'algèbre $\A$ engendrée par les intervalles $[a,b[$, c'est-à-dire les réunions finies d'intervalles de ce type. On peut supposer cette réunion disjointe.

On a une mesure additive $\underline\lambda : \A \rightarrow [0,1] $ donnée par la somme des longueurs. On peut essayer d'appliquer le théorème d'extension de Hahn - Kolmogorov pour étendre $\underline\lambda$ en une mesure borélienne $\lambda$. Ceci implique l'unicité de $\lambda$.

On peut montrer l'existence par deux méthodes : ou bien on montre que $\underline{\lambda} $ est $\sigma$-additive sur $\A$, \emph{ce qui n'est pas vrai pour toute mesure additive sur $\A$} ; ou bien, ce que nous allons faire : On pose 
\[Y := \lbrace 0,1, \ldots , 9 \rbrace \]
\[ \Omega = Y ^\N \]


\begin{align*}
f : \ &\Omega \rightarrow [0,1[ \\
&(y_i) \mapsto \sum_{i=1}^\infty y_i 10^{-i} 
\end{align*}


On met sur $\Omega$ la probabilité $\prob$ qui est le produit des lois uniformes sur Y. Alors $f_i \prob $ est la mesure cherchée.
Il suffit de vérifier que pour tout $a \in \mathbb{D} $, $f^{-1} \left( [0,a]  \right) $ est mesurable et que 
$\prob \left( f^{-1} \left( [0,a]  \right) \right) = a $ avec $ a = 0, a_1 a_2 \ldots a_k $. \\

$\lbrace f \leq a \rbrace = \lbrace y_1 < a_1 \rbrace \cup (\lbrace y_1 = a_1 \rbrace \cap \lbrace y_2 < a_2 \rbrace) \cup \dots \cup \lbrace y = a \rbrace $ \\

Ceci implique que $\lbrace f \leq a \rbrace$ est dans $ \A_{\infty} $ et $P (f \leq a ) = a_1 \cdot 10^{-1} + \dots + a_k \cdot 10 ^ {-K} + 0 = a $.

\end{demo}

\subsection{Retour à $\R$}

Si $\lambda $ est une mesure sur $\R$, alors pour tout $B \in \mathcal{B}$, $\lambda (B) = \sum \limits_{n \in \mathbb{Z} } \lambda (B \cap [n, n+1[ ) $ \\

Notons temporairement $P$ la mesure de Lebesgue sur $[n , n+1 [ $, alors on dit avoir :
\begin{equation*}
\lambda (B) = \sum \limits_{n \in \mathbb{Z} }P (B \cap [n, n+1[ )
\end{equation*}


Il reste à vérifier que la formule ci-dessus définit bien une mesure sur $\R$, l'autre propriété étant facile à vérifier. \\

Soit $B_k $ une suite de boréliens disjoints de $\R$ : 

\begin{propriete}

$\lambda$ est invariante par translation : c'est-à-dire que pour toute translation $\tau$, $\tau _* \lambda = \lambda $. 

\end{propriete}

\begin{demo}

$\left( \tau _* \lambda \right) \left( [a,b] \right)  = \lambda \left( \tau^{-1}  \left( [a,b] \right) \right) = b-a $ \\

D'où le résultat désiré par unicité.

\end{demo}

\begin{propriete}

Soit $\mu $ une mesure borélienne sur $\R$, invariante par translation telle que $\mu ([0,1[ )< \infty $, alors $\mu = \mu ([0,1[ ) \cdot \lambda )$.

\end{propriete}

\begin{demo}
	Tous les points ont la même mesure, et c'est $0$. Ensuite, comme $[0,1[$ est la réunion disjionte de $k$ translatés de $[0,\frac 1 k[$, on conclut que $\mu\left(\left[0, \frac 1 k\right[\right) = \frac{\mu([0,1[)}{k}$. On pose alors $\tilde \mu = \frac \mu {\mu([0,1[)}$.
	
	Si $q=\frac l k$ est un rationnel, alors $[0,q[$ est l'union disjointe de $l$ translatés de $[0,\frac 1 k[$, donc $\tilde\mu([0,q[)= q$.
	
	Par passage à la limite croissante, $\tilde\mu([0,t[) = t \quad \forall t \ge 0$.
	
	Et donc, $\tilde\mu(Z)=\lambda(Z)$ pour tout intervalle, donc $\tilde\mu=\lambda$.
\end{demo}

\section{Mesures de probabilité boréliennes sur $\R$} 

Si P est une mesure de probabilité sur $\R$, on définit sa \emph{fonction de répartition} : $f(t) := P ( ]- \infty , t ] ) $.

\begin{propriete}

La fonction de répartition est 

	\begin{itemize}
		\item croissante ;
		\item continue à droite ;
		\item de limite nulle en moins l'infini et de limite unitaire en plus l'infini.
	\end{itemize}

\end{propriete}

\begin{theorem}
Si $f$ est une fonction vérifiant les trois propriétés ci-dessus, alors il existe une unique probabilité borélienne $\prob$ dont $f$ est la fonction de répartition.
\end{theorem}

\begin{demo}
\begin{itemize}
	\item Unicité : si $\prob$ et $\tilde\prob$ engendrent la même fonction de répartition, alors elles ont les mêmes valeurs sur les intervalles de la forme $]a,b]$ donc sur tous les boréliens.
	\item Existence : on considère une fonction $g$ qui va jouer le rôle d'"inverse" de $f$ : \[g(\omega) := inf \lbrace x \in \R ; f(x) \geq \omega \rbrace\]
	La condition 3 implique que pour tout $\omega$ dans $]0,1[$, $g(\omega ) \in \R$ et $g$ est croissante, donc pour tout $t \in \mathbb{R} $, $ g(f(t)) \leq t$. 	
\end{itemize}
\end{demo}

\begin{lemme}
Toute fonction croissante de $]0,1[$ dans $\R$ est mesurable 
\end{lemme}

\begin{demo}

Si I est un intervalle de $\mathbb{R}$, alors $ g^-1 (I) $ est un intervalle car si $x,y \in g^-1 (I) $ alors $[x,y] \in f^-1 (I) $. Comme les intervalles engendrent la tribu, on conclut que $g$ est mesurable. 

\end{demo}

Il reste à vérifier que $g_*  \lambda$ a bien $f$ comme fonction de répartition. Dans le cas où $f$ est inversible ($f$ est continue et strictement croissante ), alors $g = f^-1$.

Dans le cas général, on a encore $g^{-1}(]-\infty, t]) = ]0,f(t)]$, $\omega \le f(t) \Rightarrow g(\omega) \le g \circ f(t) \le t$. Réciproquement, 

\section{Intégration}

On cherche à définir $\int_\R f(t)\mathrm dt$, et plus généralement $\int_\Omega f(\omega)\mathrm dm(\omega)$ lorsque $(\Omega,\T, m)$ est un espace mesuré, et $f:(\Omega,\T)\rightarrow(\R,\borel)$ est mesurable.

\begin{propriete}
	Si $f, g$ sont mesurables, $a\in\R$, alors, $f+g$, $af$, $fg$, $\min(f,g)$, $\max(f,g)$ sont mesurables.
\end{propriete}

\begin{demo}
	$\R^2$ muni de sa tribu $\borel$. $(f,g) : \Omega \rightarrow \R^2$ est mesurable donc si $h:\R^2 \rightarrow R$ est mesurable alors $h(f,g)$ est mesurable.
	
	Or une fonction continue est mesurable (caractérisation des boréliens par les ouverts).
\end{demo}

\begin{rem}
	Moyennant que cela ait un sens, ce qui n'est pas toujours le cas, cette propriété s'étend au cas de fonctions à valeurs dans $[-\infty,+\infty]$.
\end{rem}

\begin{rem}
	Une fonction $f : \Omega \rightarrow \bar\R$ est mesurable ssii les préimages $f^{-1}(I)$ sont mesurables ou si $f^{-1}(\R)$, $f^{-1}(+\infty)$, $f^{-1}(-\infty)$ sont mesurables et $f|_{f^{-1}(\R)} : f^{-1}(\R)\rightarrow \R$ est mesurable.
\end{rem}

\begin{propriete}
	Soit $(f_n)$ une suite de fonctions mesurables, à valeurs dans $\bar\R$, alors $\sup f_n$, $\inf f_n$, $\liminf f_n$, $\limsup f_n$ sont mesurables (donc en particulier $\lim f_n$ quand elle existe).
\end{propriete}

\begin{demo}
	$\{\sup f_n > a\} = \bigcap_n \{f_n > a\}$ est mesurable. On conclut par passage à l'opposé et par composition de $\sup$ et d'$\inf$.
\end{demo}

\begin{rem}
	Surtout en analyse, on se préoccupe rarement de vérifier si quelque chose est mesurable ou pas, parce que presque tout ce qu'on a envie de considérer est mesurable.
\end{rem}

\begin{rem}
	Si $(f_\alpha)$ est une famille quelconque de fonctions mesurables, $\underset\alpha\sup f_\alpha$ n'est pas fonrcément une fonction mesurable.
	
	\begin{exemple}
		On considère $f : x \in \R \mapsto \delta_{0,x}$ (symbole delta de Kronecker). Toute fonction $g : \R \rightarrow ]0,1[$ est le $\sup$ d'une famille de fonctions de la forme $(x\mapsto f(x-a)-b)_{a,b}$.
	\end{exemple}
\end{rem}

\begin{propriete}
	\begin{enumerate}
		\item Si $f:\Omega \rightarrow [0,+\infty]$ est mesurable alors $f$ est la limite croissante d'une suite $(f_n)$ de fonctions qui prennent un nombre fini de valeurs réelles positives (a.k.a des fonctions simples).
		\item Si $f: \Omega \rightarrow \R$ est mesurable, alors c'est la limite uniforme d'une suite $(f_n)$ de fonctions prenant un nombre dénombrable de valeurs réelles.
	\end{enumerate}
\end{propriete}

\begin{demo}
	\begin{enumerate}
		\item Pour chaque $n$, on pose $f_n = \frac k {2^n}$ si $k \le 4^k$ et $f\in[\frac k {2^n}, \frac {k+1}{2^n}[$ et $f_n = 2^n$ si $f \ge 2^n$.
		\begin{quote}
			\textit{On fait du coloriage pour enfants vraiment jeunes, on s'en fiche de dépasser, les parents seront quand même contents à la fin.} - Thibaud \textsc{Raymond}
		\end{quote}
		Alors $f_n$ est croissante, $f_n \le f$, $f_n \ge f-\frac 1 {2^n}$ sauf si $f\ge 2^n$ donc dans tous les cas $\forall x, f_n(x) \rightarrow f(x)$
		\item On ne s'en servira pas donc on ne le démontre pas.
	\end{enumerate}
\end{demo}

\begin{definition}[Intégrale d'une fonction positive]
	\begin{enumerate}
		\item Si $f$ prend un nombre fini de valeurs : $\int f\dd m := \underset{y\in \Im(f)}{\sum} y\cdot m(f=y)$. C'est une somme finie, mais certains termes peuvent être infinis.
		
		L'intégrale ici est bien définie, d'après la propriété précédente, pour
		\begin{itemize}
			\item Si $f$ est à valeurs dans $[0,+\infty]$
			\item Si $f$ est à valeurs réelles si $m$ est finie, même lorsque $f$ n'est pas positive.
		\end{itemize}
	\end{enumerate}
\end{definition}

On va noter $S$ l'ensemble des fonctions mesurables réelles prenant un nombre fini de valeurs (les fonctions simples), et $S^+$ celles qui sont positives.

\begin{propriete}
	\begin{itemize}
		\item Si $m$ est finie, alors $f\in S \mapsto \int f\dd m$ est linéaire.
		\item En général, pour tout $m$, $\int af + bf = a\int f + b\int g, \quad \forall a,b \ge 0, \forall f,g \in S^+$.
	\end{itemize}
\end{propriete}

\begin{definition}
	Pour $f: \Omega \rightarrow [0,+\infty]$ mesurable, on définit $\int f\dd m = \underset{\underset{g \le f}{g \in S^+}}{\sup} \int g \dd m$
\end{definition}

\begin{propriete}
	\begin{enumerate}
		\item $0 \le f \le \tilde f \Rightarrow \int f \le \int \tilde f$
		\item $0 \le f$ et $\int f \dd m = 0 \Rightarrow m(f>0) = 0$ (on dit $f = 0$ p.p.)
		\item $f=0$ p.p., $f\ge0 \Rightarrow \int f\dd m = 0$
		\item $\int_A f\dd m_A = \int_\Omega f\mathds 1_A \dd m$ si $m_A(B):=m(A\cap B)$.
	\end{enumerate}
\end{propriete}

\begin{demo}
	\begin{enumerate}
		\item Vient trivialement avec 2.
		\item $0=\int f\dd m \ge \int \eps \mathds 1_{\{f\ge \eps\}} \dd m =\eps m(f\ge \eps)$
		
		$\forall \eps > 0, m(f\ge \eps) = 0$. On prend l'intersection sur $\eps = \frac 1 n$. Donc $m(f>0) = 0$.
		\item Soit $g\in S^+$ telle que $0 \le g\le f$ et $f=0$ p.p. $\Rightarrow g =0$ p.p. $\Rightarrow \int g \dd m = 0$
		\item Si $g \in S$, $\int_A g \dd m_A = \underset{y \in \Im g}\sum y\cdot m_A(g=y)$
		
		$\int_\Omega g \mathds 1_A \dd m = \underset{y\in\Im g\mathds 1_A}\sum y \cdot m(g\mathds 1_A=y) = \underset{y\in \Im y \cup \{0\}}\sum y\cdot m(A\cap \{g=y\})$
	\end{enumerate}
\end{demo}

\begin{theorem}[Convergence monotone]
	Si $(f_n)$ est une suite de fonctions mesurables à valeurs dans $[0,+\infty]$ croissante, alors 
	\[ \int f_n \dd m \underset{n\rightarrow + \infty} \longrightarrow \int \underset{n\rightarrow + \infty} \lim f_n \dd m \]
\end{theorem}

\begin{demo}
	$f_n \le f$ donc $\int f_n \le \int f \forall n$ donc $\lim \int f_n \le \int f$
	Il faut démontrer que $\lim \int f_n \ge \int f$ ce qui découle de $\lim \int f_n \ge \int g \quad \forall g\in S^+, g\le f$
	On se fixe $\lambda \in ]0,1[$, et on définit $g_n := \left\{ \begin{aligned}
		&\lambda g &\text{si } f_n\le \lambda g \\
		& 0 &\text{si } f_n < \lambda g
	\end{aligned}\right.$

	$g_n \le f_n$, $g_n$ est croissante et $g_n \rightarrow \lambda g$. On avait démontré le théorème de convergence monotone pour les fonctions simples.
	$\int g_n \dd m \rightarrow \int \lambda g \dd m$.
	$f_n \ge g_n \Rightarrow \lim\int f_n \ge \lim \int g_n \ge \int \lambda g \dd m$
	
	On a montré : $\forall \lambda \in ]0,1[, \quad \lim \int f_n \ge \int \lambda g = \lambda \int g$. Donc, par passage au $\sup$ sur $\lambda$, $\lim \int f_n \ge \int g \quad, \forall g \in S^+, g \le f$, et donc $\lim \int f_n \ge \int f$.
\end{demo}

\begin{lemme}[Fatou]
	Soit $(f_n : \Omega \rightarrow [0,+\infty])_n$ une suite de fonctions mesurables.
	
	Alors $\int \liminf f_n \le \liminf \int f_n$.
\end{lemme}
\begin{rem}
	\begin{enumerate}
		\item cf lemme de Fatou ensembliste.
		\item Penser aux fonctions triangles pour un exemple où l'inégalité n'est pas respectée (une bosse qui part à l'infini).
	\end{enumerate}
\end{rem}

\begin{demo}
	$g_n = \underset{k\ge n}\inf f_k$. $g_n$ est croissante, $\liminf f_n := f$. On montre par le théorème de convergence monotone $\int g_n \rightarrow \int f$, et $g_n \le f_n$ donc $\liminf \int f_n \le \lim \int g_n = \int f$.
\end{demo}

\begin{propriete}
	Si $a,b \ge 0$, $f,g \ge 0$ mesurables, alors $\int(af+bg) = a\int f + b\int g$
\end{propriete}
\begin{demo}
	Soient $f_n, g_n \in S^+$ qui tendent simplement vers $f$ et $g$ respectivement.
	
	Alors $af_n+bg_n \in S^+$, est croissante et tend vers $af+bg$.
	
	Par théorème de croissance monotone, $\int f_n \rightarrow \int f$, $\int g_n \rightarrow \int g$, $\int af_n+bg_n \rightarrow \int af+bg$.
	
	De plus $_int af_n+bg_n = a\int f_n + b\int g_n$, donc on conclut par passage à la limite.
\end{demo}

\begin{propriete}
	Si $f:\Omega \rightarrow [0,+\infty]$ mesurable, alors la fonction $A \mapsto \int_A f\dd m, \quad A\in \T$ est une mesure sur $(\Omega,\T)$. On la note parfois $fm$.
\end{propriete}
\begin{demo}
	On note $\mu(A) = \int_A f\dd m$.
	
	Soient $A,B\in \T, A\cap B = \varnothing$. Alors $\mathds 1_{A\sqcup B} = \mathds 1_A + \mathds 1_B$
	
	$\mu(A\sqcup B) = \int_\Omega f\mathds 1_{A\sqcup B} \dd m = \int_\Omega f \mathds 1_A \dd m + \int_\Omega f \mathds 1_B \dd m = \mu(A)+\mu(B)$


$\sigma$-additivité : $(A_n) \in \T^\N$ disjoints. On pose $B_n = \bigsqcup_{i=1}^n A_i$

$\mu(B_n)=\sum_{i=1}^n \mu(A_i)$ or $\sum_{i=1}^n \mu(A_i) \rightarrow \sum_{i=1}^\infty \mu(A_i)$.

$\mu(B_n) = \int_\Omega f \1_{B_n} \dd m \rightarrow \int_\Omega f \1_B \dd m$ (convergence monotone).

$f\1_{B_n}$ tend en croissant vers $f\1_B$
\end{demo}

\begin{definition}[Fonctions mesurables à valeurs réelles]
	
	$f^+ := \max(f,0), \quad f^- := max(-f, 0)$
	
	$|f| = f^+ + f^-, \quad f=f^+-f^-$
	
	On a envide définir $\int f := \int f^+ - \int f^-$. Ceci a un sens si l'une des intégrales de $f^+$ ou $f^-$ est finie.
	
	On dit que $f$ est \emph{intégrable} si elle est mesurable et si $\int f^+$ et $\int f^-$ sont finies, c'est à dire $\int|f| < +\infty$.
\end{definition}

\.\\

\begin{definition}
	On note $\mathcal L^1(\Omega,m)$ l'ensemble des fonctions intégrales à valeurs réelles.
\end{definition}

\begin{propriete}
	$\mathcal L^1$ est un espace vectoriel et $f\mapsto \int_\Omega f \dd m$ est linéaire sur $\mathcal L^1$.
\end{propriete}

\begin{demo}
	Si $f$ et $g$ sont intégrable, alors $\int|af+bg| \dd m \le |a|\int|f| + |b|\int|g| < + \infty$.

	Donc $af+bg \in \mathcal L^1$.

	Si $_f\in\mathcal L^1, \alpha\in \R$, alors pour $a\ge 0$, $(af)^\pm = a(f^\pm)$
	
	Donc $\int(af)^+ - (af)^- = \int a(f^+) - a(f^-) = a \int f^+-f^- = a\int f$.
	
	Idem pour $a\le 0$.
	
	$f,g\in \LL^1$. A priori, $(f+g)^+ \neq f^+ + g^+$, donc on doit prendre quelques précautions.
	
	$f+g = (f+g)^+ - (f=g)^- = f^+ - g^- + g^+ - g^-$. Le reste suit trivialement.
	
\end{demo}

\begin{theorem}[Convergence dominée]
	Soit $(f_n : (\Omega, \T, m) \rightarrow \R)_n$ une suite de fonctions mesurables telles que :
	\begin{itemize}
		\item $f_n \rightarrow f$ presque partout.
		\item $\exists h\ge$ intégrable telle que $|f_n| \le h$ presque partout pour tout $n$.
	\end{itemize}

	Sous ces hypothèses, $\int f_n \rightarrow \int f$
\end{theorem}

\begin{demo}
	On applique le lemme de Fatou à $f_n+h$ et à $h-f_n$.
	\begin{itemize}
		\item $f_n \rightarrow f$ presque partout donc $\int f \le \liminf \int(f_n+h) \dd m = (\liminf \int f_n) + \int h$
		
		Donc $\int f \le \liminf \int f_n$
		\item $h-f_n \rightarrow  h-f$ (et sont positives)
		
		$\int h-f \le \liminf \int h - f_n \dd m = \int h - \limsup \int f_n \dd m$
		
		Donc $\int f \ge \limsup \int f_n \dd m \ge \liminf \int f_n \ge \int f$.
		
		Donc $\int f = \lim \int f_n$.
	\end{itemize}
\end{demo}

\begin{definition}[L'espace $\LL^1$]
	On définit $\|f\|_1 = \int|f|\dd m$ pour tout $f$ mesurable.
	
	$f\in \LL^1 \Leftrightarrow \|f\|_1 < + \infty$.
	
	$\|\cdot \|_1$ est une semi-norme (ne vérifie pas l'hypothèse de séparation).
\end{definition}

Pour y remédier, on "identifie" (on quotiente par) les fonctions égales presque partout.

On note $L^1$ ce quotient.

\begin{rem}
	Attention, pour $f\in L^1$, $f(x)$ n'est pas défini, sauf si $m({x}) > 0$.
\end{rem}

\begin{propriete}
	$L^1$ est un espace vectoriel :
	$f=\tilde f$ p.p., $g=\tilde g$ p.p. alors $af+bg = a\tilde f + b\tilde g$ p.p.
\end{propriete}
\begin{demo}
	Évident.
\end{demo}

\begin{propriete}
	$f = \tilde f  \text{ p.p.}\Rightarrow \|f\|_1 = \|\tilde f\|_1$, et $\|\cdot \|_1$ induit une norme sur $L^1$, qu'on notera pareil.
	
	Donc $(L^1(\Omega,\T,m), \|\cdot\|_1)$ est un espace vectoriel normé.
\end{propriete}

\begin{rem}
	C'est une propriété générale : quotienter un EV par le noyau d'une semi-norme donne un espace vectoriel normé (on note $\pi$ la surjection canonique).
\end{rem}

\begin{propriete}
	L'ensemble $S$ des fonctions simples est dense dans $L^1$, plus exactement $S\cap L^1$ est dense dans $L^1$
\end{propriete}

\begin{rem}
	Plus exactement $\pi(S\cap\LL^1)$ est dense dans $L^1$.
\end{rem}

\begin{demo}
	Posons $f\in \LL^1$, $f=f^+ - f^-$.
	
	$\exists(f_n^+),(f_n^-)$ des suites croissantes de fonctions simples positives qui convergent vers $f^+$ et $f^-$.
	
	Montrons que $f_n^+-f_n^- \underset{\LL^1}{\rightarrow} f$. Par convergence monotone : $\int f_n^+ \rightarrow f^+, \int f_n^- \rightarrow f^-$.
	
	Donc $\|f-(f_n^+-f_n^-)\|_1 \le \|f^+-f_n^+\|_1 + \|f^- - f_n^-\|_1 = \int f^+ - f_n^+ + \int f^--f_n^-  \rightarrow 0$ 
\end{demo}

Le théorème de convergence dominée n'est pas optimal mais :

\begin{propriete}
	Si $f_n \rightarrow f$ dans $L^1$ alors ils existe une sous-suite $f_{n_k}$ qui satisfait les hypothèses du TCD : 
	\begin{enumerate}
		\item $f_{n_k} \rightarrow f$ p.p.
		\item $\exists h \ge 0$ intégrable telle que $|f_{n_k}| \le h$ presque partout pour tout $k$.
	\end{enumerate}
\end{propriete}

\begin{demo}
	On choisit $n_k$ telle que $\|f_{n_k}-f\| \le k^{-3}$.
	
	Alors $m(|f_{n_k}-f| \ge \frac 1 k) \le k ^{-2}$ par Markov.
	
	$\sum k^{-2} < +\infty$ donc on peut appliquer le premier lemme de Borel-Cantelli.
	
	Donc $m(|f_{n_k}-f| \ge \frac 1 k \text{i.o.}) = 0$, et en dehorsde cet ensemble, $f_{n_k} \rightarrow f$, donc on a 1.
	
	Pour 2., on pose $g = \sum |f_{n_k + 1} - f_{n_k} : \Omega \rightarrow [0,+\infty]$.
	
	$g_l = \sum_{k=1}^l |f_{n_k+1} - f_{n_k}| ...$ %TODO
	
	$\|g\|_1 = \int g \underset{\text{CV monotonne}} = \lim \int g_l \le \lim \sum_{k=1}^l \|f_{n_k+1} - f_{n_k}\| \le \sum_{k=1}^\infty k^{-3} < +\infty$
	
	Or, $|f_{n_k}| \le |f| + g$ car $|f_{n_k} - f| = \sum_k^\infty|f_{n_{k+1}} - f_{n_k}| \le g$
\end{demo}

\begin{rem}
	La mesure de Lebesgue sur $\R$ n'est pas finie mais $\sigma$-finie.
\end{rem}

\begin{lemme}
	Sur $(\Omega,\T)$, la mesure $m$ est $\sigma$-finie ssi il existe une probabilité $\prob$ et une fonction $f:\Omega \rightarrow [0,+\infty[$ mesurable telle que $m=f\prob$, où $f\prob$ est la mesure $A\mapsto \int_A f\dd\prob$
\end{lemme}

\begin{demo}
	Si $m=f\prob$, on définit $A_n=\{f\le n\}$.
	$\Omega = \underset n \bigcup A_n$ et 
	\[m(A_n) = \int_{A_n}f\dd\prob \le \int_{A_n}n\dd\prob \le n\prob(A_n) \le n\]
	
	Réciproquement, si $m$ est $\sigma$-finie, il existe $(A_n)$ telle que $\Omega = \underset n \bigcup A_n$, $m(A_n) < \infty$. On pose $B_n = A_n \setminus \underset{k=1} {\overset {n-1} \bigcup} A_n$. Alors $m(B_n) \le m(A_n) < \infty, \bigsqcup B_n = \bigcup A_n = \Omega$ et les $B_n$ disjoints.
	
	On va considérer une fonction $f=a_n$ sur $B_n$, $a_n \in \R_+$. On définit $\prob := \frac 1 f m$.
	
	$P(\Omega) = \underset{k=1}{\overset \infty \sum} P(B_k) = \underset{k=1}{\underset \infty \sum} \frac 1 {a_k} m(B_k)$.
	
	On peut prendre $a_k=m(B_k)\cdot 2^{-k-1}$. Alors, $P(\Omega) = \sum_{k\ge 1} 2^{-(k+1)} = 1$.
\end{demo}

\begin{rem}
	La mesure de Lebesgue est "localement finie", i.e. tout point admet un voisinage de mesure finie. On dit aussi que $\lambda$ est une "mesure de Radon".
	
	Mais ce concept concerne la structure topologique et la structure mesurée de $\R$.
\end{rem}

\section{Intégration et mesure image}
	$(\Omega,\T,m)$ et $\phi:\Omega \rightarrow X$ alors on a défni une tribu image $\phi_*\T = \{Y\subset X,\ \phi^{-1}(Y)\in\T\}$ et la mesure image (loi) $\phi_*m(Y) = m(\phi^{-1}(Y))$.

\begin{propriete}
	La mesure $\mu=\phi_*m$ vérifie $\int f\dd\mu = \int f \circ \phi \dd m$ pour toute fonction $f$ positive telle que $f\circ \phi$ est mesurable sur $(\Omega,\T)$.
\end{propriete}
\begin{rem}
	\begin{enumerate}
		\item Cette relation est vraie aussi pour $f$ à valeurs réelles si $f\circ\phi$ est $m$-intégrable.
		\item Cette propriété caractérise $\mu$ car $\mu(Y)=\int\1_Y\dd\mu = \int \1_Y \circ \phi \dd m = \int \1_{\phi^{-1}(Y)}\dd m = m(\phi^{-1}(Y))$.
	\end{enumerate}
\end{rem}

\begin{demo}
	Pour quelles fonctions $f$ a-t-on $\int f\dd \mu = \int f \circ \phi \dd m$ ?
	\begin{enumerate}
		\item Pour les indicatrices de $Y$ avec $Y \subset \phi_*\T$.
		\item Par linéarité, pour toute fonction simple positive $\phi_*\T$-mesurable.
		\item Pour toute fonction $\phi_*\T$-mesurable positive. Si $f$ est une telle fonction, on considère $(f_n)$ croissante de fonctions simples, $f_n \rightarrow f$.
		
		On peut passer à la limite dans l'égalité $\int f_n \dd \mu = \int f_n \circ \phi \dd m \overset{\text{CV monotone}} \longrightarrow \int f \dd\mu = \int f\circ \phi \dd m$.
	\end{enumerate}
\end{demo}

\begin{propriete}
	Si $\mu = fm$ et si $g$ est mesurable positive alors $\int g \dd \mu = \int fg \dd m$.
\end{propriete}

\begin{rem}
	$\prob = \frac 1 f m \Rightarrow m=f\prob$ (même preuve)
\end{rem}

\section{Intégrales dépendant d'un paramètre}

On étudie la fonction $F : x \mapsto \int_\Omega f(x,\omega) \dd m(\omega)$, où $f:X\times \Omega \rightarrow \R$, $f(x, \cdot)$ intégrable sur $(\Omega,\T,m)$ pour tout $x$.

\subsection{Continuité}
On suppose que $X$ est un espace métrique.
\begin{theorem}
	Si : \begin{itemize}
		\item $x\mapsto f(x,\omega)$ est continue en $x_0$ pour presque tout $\omega$
		\item $|f(x,\omega)| \le g(\omega) \quad \forall x$ avec $g$ intégrable.
	\end{itemize}
	Alors $F$ est continue en $x_0$.
\end{theorem}
\begin{demo}
	Si $x_n \rightarrow x_0$, alors $h_n(\omega) := f(x_n,\omega)$ converge presque partout vers $h(\omega)=f(x_0,\omega)$, et $|h_n(\omega)| \le g$.
	
	Théorème de convergence dominée : $F(x_n) ......$
\end{demo}

\begin{exemple}
	\begin{enumerate}
		\item Si $m$ est une mesure finie sur $\R$, on défjnit sa transformée de Fourier (fonction caractéristique en proba) : $\hat m(y) := \int e^{iyx}\dd m(x)$.
		$y\mapsto \hat m(y)$ est continue sur $\R$, par le théorème qui précède :
		$y\mapsto e^{iyx}$ est $\mathcal C^0 \quad \forall x$ et $|e^{iyx}|\le 1$, $1$ est intégrable.
		\item Convolutions. $f \in \LL^1(\R,\lambda)$ et $h$ continue et bornée. On définit $h\varhexstar f : x \mapsto \int h(x-t)f(t)\dd \lambda(t)$, $x\in\R$.
		$h\varhexstar f$ est continue.
		
		$x,t\mapsto |h(x-t)f(t)| \le \ (\sup h)|f|$
		\begin{rem}
			Si $h$ et $f\ge 0$, alors $\forall x\in\R, h\varhexstar f(x) = f\varhexstar h(x) = \int f(x-t)h(t) \dd t$.
			$\int_\R h(x-t)f(t)\dd\lambda(t) = \int_\R h(-s)f(s-x) \dd\lambda(s)$ car $\lambda$ est invariante par translation.
			$= \int_\R h(s)f(x-s)\dd\lambda(s)$ car $\lambda$ est invariante par la symétrie $s\mapsto -s$ (ne change pas la longueur des intervalles).
		\end{rem}
	\item $f\in\LL^1 \Rightarrow x\mapsto \int_{-\infty}^x f(t) \dd t \in \mathcal C^0$.
	\begin{demo}
		$F : x\mapsto \int _\R f(t)\1_{]-\infty,x]}(t)\dd t$. La fonction intégrée est dominée par $|f(t)|$ et $\forall x_0, x\mapsto f(t)\1_{]-\infty,x]}(t)$ est continue en $x_0$, $\forall t \neq x_0$ or $\lambda({x_0}) = 0$. Donc $F$ est continue en $x_0$.
	\end{demo}
	\end{enumerate}
\end{exemple}
\begin{rem}
	Notation : $\int_A f(t)\dd \lambda(t)$ / $\int_a^b f(t)\dd t$.
	\begin{definition}
		$\int_a^b f(t) \dd t = \left\{\begin{aligned}
			&\int_{[a,b]} f(t)\dd\lambda(t)&\text{ si } a\le b\\
			&-\int_{[a,b]}f(t)\dd \lambda(t)&\text{ si } a > b
		\end{aligned} \right.$
	\end{definition}
\end{rem}

\subsection{Dérivabilité}
$X\subset \R$ est un intervalle ouvert.
\begin{theorem}
	\begin{itemize}
		\item $\forall x\in X, f(x,\cdot)$ est intégrable.
		\item $x\mapsto f(x,\omega)$ est dérivable en $x_0$, $\forall \omega\in\Omega$
		\item $\exists g\in \LL^1, |f(x,\omega)-f(x_0,\omega)| \le |x-x_0|g(\omega), \quad \forall x\in X, \omega \in \Omega$.
	\end{itemize}
	Alors $F$ est dérivable en $x_0$ et $F'(x_0) = \int_\Omega\partial_xf(x_0,\omega)\dd m(\omega)$.
\end{theorem}
\begin{rem}
	$|f(x,\omega)-f(x_0,\omega)|\le |x-x_0|g(\omega)$ est satisfaite si $\partial_xf$ existe partout et $|\partial_xf(x,\omega)|\le g(\omega), \quad \forall x\in X, \omega \in \Omega$.
\end{rem}
\begin{demo}
	$\frac{F(x_n)-F(x_0)}{x_n-x_0} = \int_\Omega \frac{f(x_n,\omega)-f(x,\omega)}{x_n-x_0} \dd m(\omega)$
	
	$x_n \rightarrow x_0$.
	
	On peut appliquer le TCD : $\int_\Omega \partial_x f(x_0,\omega)\dd m(\omega)$.
\end{demo}

\begin{exemple}
	\begin{enumerate}
		\item $f\in \LL^1$, $h$ est $\mathcal C^1$, bornée de dérivée bornée, alors $f\varhexstar h$ est $\mathcal C^1$ et $(f\varhexstar h)'f\varhexstar (h')$.
		\begin{demo}
			$\int h(x-t)f(t)\dd t$, $g(x,t)=h(x-t)f(t)$, $\partial_x g = h'(x).....$
		\end{demo}
		\item Si $m$ est une mesure finie et $\int |x| \dd m < +\infty$ alors $\hat m$ est $\mathcal C^1$ et $\hat m' = \int ixe^{iyx}\dd x$
		\begin{demo}
			$f(y,x) = e^{iyx}$, $\partial_y f = iye^{iyx}$, $|\partial_y f| \le |x|$.
		\end{demo}
	\end{enumerate}
\end{exemple}

\begin{propriete}[Théorème fondamental de l'analyse]
	Si $f$ est continue, $F : x \mapsto \int_0^x f(t)\dd t$ est dérivable de dérivée $f(x)$.
\end{propriete}
\begin{demo}
	$F(x_0+x) - F(x_0) =  \int_{x_0}^{x_0+x} f(t)\dd t \le \int_{x_0}^{x_0+x} f(x_0) + \eps(|x|) \dd t \le f(x_0)x + |x|\eps(|x|)$. De même, $F(x_0+x) - F(x_0) \ge \int_{x_0}^{x_0+x}(f(x_0)-\eps(|x|))\dd t \le f(x_0)x-|x|\eps(|x|)$.
	
	$\eps(x) = \underset{t\in [x_0-x, x_0+x]}\sup |f(t)-f(x_0)|$
	
	$f(t) \le f(x_0) +\eps(x) \forall t [x_0-x, x_0+x]$.
\end{demo}
\subsection{Inégalité de Jensen}

	$\phi:\R\rightarrow \R$ convexe ($\forall t \in [0,1],\forall x,y\in\R, \phi(tx+(1-t)y)\le t\phi(x)+(1-t)\phi(y), \phi : \R \rightarrow \bar\R)$
On appelle domaine de $\phi$ l'ensemble $\{\phi<+\infty\} \subset \R$
\begin{rem}
	Si $J\subset \R$ est un intervalle, et si $\phi : J \rightarrow \bar\R$ est convexe, alors $\tilde\phi  = \phi \text{ sur } J, +\infty \text{ hors de } J$ est convexe.
\end{rem}

\begin{theorem}[Inégalité de Jensen]

Si $\mu$ est une mesure de probabilité sur $\Omega$, et $\phi : \R \rightarrow \bar\R$ est convexe, alors $\forall f$ mesurable, intégrable (ou semi-intégrable), $\phi\left(\int_\Omega f \dd\mu\right) \le \int_\Omega \phi \circ f \dd\mu$.
\end{theorem}
(on dit que $f$ est semi-intégrable si $\int f^+ \dd\mu < +\infty$ ou $\int f^- \dd\mu < + \infty$).
\begin{rem}
	Si $f$ est semi-intégrable mais pas intégrable, alors $\int f\dd\mu \in \{-\infty, + \infty\}$. Alors $\phi\left(\int f\dd\mu\right) := \underset{t\rightarrow \pm\infty} \lim \phi(t)$
\end{rem}
\begin{rem}
	$\phi(txx+(1-t)y) \le t\phi(x)+(1-t)\phi(y) \Leftrightarrow \phi\left(\int \omega \dd\mu\right) \le \int \phi(\omega)\dd\mu$ où $\mu = t\delta_x + (1-t)\delta_y$.
	
	Par récurrence sur le nombre de points, on montre que si $\phi$ est convexe, alors $\phi(\mathrm{Bar(x_1, \dots, x_n, t_1, \dots, t_n)}) \le \mathrm{Bar(\phi(x_1), \dots, \phi(x_n), t_1, \dots, t_n)}$ où $\mathrm{Bar}$ désigne le barycentre.
	
	On pourrait donner une preuve de Jensen sur cette base. On fait autrement.
\end{rem}
\begin{propriete}
	Si $\phi : \R \rightarrow \R\cup\{+\infty\}$ est convexe.
	\begin{enumerate}
		\item Le domaines de $\phi$ est un intervalle $I$.
		\item $\phi$ est continue sur $\mathring I$ et admet des dérivées et à droite en chaque point de $\mathring I$ telle que $\phi'(x^-) \le \phi'(x^+) \forall x\in \mathring I$
		\item $\forall x_0 \in \mathring I$, il existe une fonction affine $l(x)$ telle que $l(x_0) = \phi(x_0)$ et $l(x) \le \phi \forall x$.
		\item $\underset{\pm \infty} \lim \phi$ existe.
	\end{enumerate}
\end{propriete}
\begin{demo}
	Fixons $x_0 \in \mathring I$, alors $p:x\mapsto \frac{\phi(x) - \phi(x_0)}{x-x_0}$ est croissante (découle de la convexité, laissée en exercice au lecteur).
	
	Alors $\underset{x\rightarrow x_0^+} \lim $ et $\underset{x \rightarrow x_0^-}\lim $ existent donc, et $p(x_0^-) \le p(x_0^+)$. De plus, ces limites sont finies : si $x_1 \le x_0 \le x_2$ dans $I$, alors $-\infty\le p(x_1) \le p(x_0^-) \le p(x_0^+) \le p(x_2) < +\infty$.
	
	Pour $x_0 \in I$, on considère $a\in [\phi'(x_0^-), \phi'(x_0)^+]$.
	$l(x) := a(x-x_0) +\phi(x_0)$. $l(x_0) = \phi(x_0)$.
	
	$l(x) \le \phi(x) \Leftrightarrow a(x - x_0) \le \phi(x) - \phi(x_0)$, ce qui est vrai par croissance du taux de variation : si $x\ge x_0, a\le \frac{\phi(x)-\phi(x_0)}{x-x_0}$.
	
	Existence des limites.
	Si $\exists x\in \mathring I, \phi'(x_0^+) > 0$, alors $\phi(x) \ge \phi(x80) + \phi'(x_0^+)(x-x_0) \forall x\ge x_0$ donc $\phi \underset {+\infty} \rightarrow +\infty$
	
	Si $\forall x_0 \in \mathring I, \phi'(x_0) \le 0 \Rightarrow \phi$ est décroissante $\Rightarrow \phi$ admet une limite.
	
	Idem en $-\infty$.
\end{demo}

\begin{demo}[Jensen]
	$I$ le domaine de $\phi$.
	
	Cas principal : $y_0 = \int f\dd\mu \in \mathring I$.
	
	$\exists l : y \mapsto ay+b$ tel que $l(x_0) = \phi(y_0), l\le \phi$.
	
	$\phi(y_0) = l(y_0) = l\left(\int f\dd\mu\right) = a \int f\dd\mu + b = \int af+b\dd\mu = \int l\circ f \dd\mu \le \int \phi\circ f \dd\mu$.
	
	Cas où $y_0 = \int f\dd\mu = \sup I$. Si $\int \phi \circ f \dd\mu = +\infty$, on n'a rien à démontrer.
	
	Si $\int \phi\circ f \dd\mu < + \infty$, alors $\phi\circ f < +\infty$ presque partout, donc $f\in I$ presque partout, donc $f\le y_0$ presque partout. Alors, $y_0-f \ge 0$ et $\int y_0-f = 0$ donc $f=y_0$ presque partout. Donc $\phi\circ f = \phi(y_0)$ presque partout donc $\int \phi \circ f = y_0$.
	
	Idem si $y_0 = \inf I$.
	
	Dernier cas : $y_0 \neq \bar I$.
	Alors $\int \phi \circ f \dd \mu = +\infty$ car $\mu(\phi\circ f = +\infty) > 0$. En effet, sinon $\phi\circ f < +\infty$ presque partout donc $f\in I$ presque partout, donc $\int f\dd\mu \in \bar I$.
\end{demo}

\section{L'espace $L^2$}

On se donne $(\Omega,\T, m)$ mesuré.

\begin{definition}
	$\LL^2(\Omega,\T,m)$ est l'ensemble des fonctions mesurables de carré intégrable ($\int f^2 \dd m < +\infty$).
\end{definition}

\begin{rem}
	Si $m$ est finie, $\LL^2 \subset \LL^1$ car $|f|\le 1+f^2$.
\end{rem}

On note $\|f\|_2 = \sqrt{\int f^2 \dd m}$, $<f,g> = \int fg \dd m$.

\begin{lemme}
	$\LL^2$ est un espace vectoriel et $f,g\mapsto <f,g>$ est une forme bilinéaire symétrique positive.
\end{lemme}
\begin{demo}
	$(f+g)^2 \le 2f^2 + 2g^2$, $()\lambda f)^2 \le \lambda^2 f^2$, donc $\LL^2$ est un sous-espace vectoriel des fonctions mesurables.
	
	$<\cdot, \cdot>$ est clairement bilinéaire, symétrique et positive. Il faut juste montrer que c'est bien défini.
	
	$|fg| \le \frac 1 2 (f^2 + g^2)$ par inégalité arithmético-géométrique, donc $fg$ est intégrable si $f,g\in\LL^2$.
\end{demo}
Ainsi, $\LL^2$ est presque un pré-hilbertien, ce est bien mais pas top. Donc on quotiente encore une fois par la semi-norme $\|\cdot\|_2$ : $L^2 = \LL^2/\|\cdot\|_2$.

Ainsi, $L^2$ est un espace pré-hilbertien, muni du produit scalaire $<\cdot,\cdot>$. En particulier, on a donc Cauchy-Schwarz ($<f,g> \le \sqrt{\|f\|_2\|g\|_2})$.

On a aussi l'égalité du parallélogramme ($\|f+g\|^2 +\|f-g\|^2 = 2(\|f\|^2 + \|g\|^2)$)

\begin{definition}
	Un espace de Hilbert est un espace pré-hilbertien complet (i.e. les suite de Cauchy convergent).
\end{definition}

\begin{propriete}
	Un espace vectoriel normé E est complet ssi les séries les séries normalement convergentes sont convergentes.
	
	$\sum\|f_n\| < +\infty \Rightarrow \sum_{k=1}^n f_k$ converge dans $E$.
\end{propriete}

\begin{demo}
	Soit $(E,\|\cdot\|)$ un evn complet, et soit $f_n$ une suite telle que $\sum\|f_n\| < + \infty$ alors $S_n := \sum_{k=1}^n f_n$ vérifie $\|X_n-S_m\| = \|\sum_{k=n+1}^m f_k\| \le \sum_{k=n+1}^m \|f_k\| \le \sum_{k=n+1}^\infty \|f_k\| = \eps(n)$ i.e. $\forall \eps, \exists n_0, \forall m\ge n_0, n\ge n_0, \|S_n-S_m\| \le \eps$ donc $S_n$ converge par complétude.
	
	Réciproque : soit $f_n$ une suite de Cauchy. Il suffit e montrer que $f_n$ a une sous-suite convergente.
	
	On choisit $n_k$ tel que : $\|f_{n_{k+1}}-f_{k_k}\|\le 2^{-k}$. Alors $\sum \|f_{n_{k+1}} - f_{n_k}\| < + \infty$ donc $\sum(f_{n_{k+1}} - f_{n_k})$ converge. Donc $f_{n_k}$ converge.
\end{demo}

\begin{theorem}
	$L^1$ et $L^2$ sont complets
\end{theorem}

\end{document}